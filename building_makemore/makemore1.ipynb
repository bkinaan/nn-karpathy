{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Makemore Park 2: Integrating a MLP\n",
    "Based off of Bengio et al. 2003 (MLP langauge model) paper but using character prediction instead of word prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in all the words\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "# build vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the dataset for the neural network\n",
    "\n",
    "# for every 3 preceding characters we are looking for a specific output character\n",
    "# this compiles the dataset into that format\n",
    "\n",
    "# build the dataset\n",
    "block_size = 3 # context lenth: how many characters do we take to predict the next one? -- chosen from the paper\n",
    "X, Y = [], []\n",
    "for w in words:\n",
    "    \n",
    "    # print(w)\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "        ix = stoi[ch]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "        # print(''.join(itos[i] for i in context), '--->', itos[ix])\n",
    "        context = context[1:] + [ix] # crop and append\n",
    "        \n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([228146, 3])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape # inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([228146])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape # labels (expected values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the neural network\n",
    "\n",
    "C = torch.randn((27, 2)) # 2 dimensional lookup table -- 27 rows and 2 colums -- each character will have a 2D embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0907, -0.7185])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of embedding one integer\n",
    "C[5] # one way to do it by indexing\n",
    "\n",
    "# from the previous lecture...\n",
    "F.one_hot(torch.tensor(5), num_classes=27).float() @ C"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both of the above ways will result in the same tensor, but we'll be using indexing simply because it is faster than one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([228146, 3, 2])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[X] # embeding X into C\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden layer\n",
    "W1 = torch.randn((6, 100)) # 100 nuerons chosen randomly\n",
    "b1 = torch.randn(100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently we have two matrices we need to multiply but they are not compatiable sizes. We have a 32,3,2 and a 6,100. We need to convert the first one to a 32,6 so that they can be multiplied. We will solve this by using torch's cat function (concatanate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0989,  0.3944,  0.0989,  0.3944,  0.0989,  0.3944],\n",
       "        [ 0.0989,  0.3944,  0.0989,  0.3944,  0.0907, -0.7185],\n",
       "        [ 0.0989,  0.3944,  0.0907, -0.7185,  0.6883, -1.9755],\n",
       "        ...,\n",
       "        [-0.4107, -0.7114, -0.4107, -0.7114,  0.0588, -0.3474],\n",
       "        [-0.4107, -0.7114,  0.0588, -0.3474, -0.4107, -0.7114],\n",
       "        [ 0.0588, -0.3474, -0.4107, -0.7114,  1.4880, -0.0618]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([emb[:, 0, :], emb[:, 1, :], emb[:, 2, :]], 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problems with this is that we are indexing directly and so if we ever wanted to change the number of context words, we would have to change this code as well (not ideal). So, we can use torch's unbind function to remove a dimension "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0989,  0.3944,  0.0989,  0.3944,  0.0989,  0.3944],\n",
       "        [ 0.0989,  0.3944,  0.0989,  0.3944,  0.0907, -0.7185],\n",
       "        [ 0.0989,  0.3944,  0.0907, -0.7185,  0.6883, -1.9755],\n",
       "        ...,\n",
       "        [-0.4107, -0.7114, -0.4107, -0.7114,  0.0588, -0.3474],\n",
       "        [-0.4107, -0.7114,  0.0588, -0.3474, -0.4107, -0.7114],\n",
       "        [ 0.0588, -0.3474, -0.4107, -0.7114,  1.4880, -0.0618]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat(torch.unbind(emb, 1), 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will work, however there is actaully an even better more efficent way to do this. We can use torch.view() to alter the dimensions of the matrix. As long as they multiply to the same value, we can shape the matrix any way we want to. This doesn't acutally alter the storage of emb, instead it just changes how that data is viewed internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[32, 6]' is invalid for input of size 1368876",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m emb\u001b[39m.\u001b[39;49mview(\u001b[39m32\u001b[39;49m, \u001b[39m6\u001b[39;49m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[32, 6]' is invalid for input of size 1368876"
     ]
    }
   ],
   "source": [
    "emb.view(32, 6)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can simply use view to alter the matrix and allow it to be multiplied by the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3417,  0.2383,  0.9875,  ...,  0.7990, -0.0490, -0.0542],\n",
       "        [-0.1474,  0.5239,  0.7186,  ...,  0.5782, -0.0049,  0.0055],\n",
       "        [ 0.8889,  0.6177, -0.8485,  ...,  0.3921, -0.7278,  0.7654],\n",
       "        ...,\n",
       "        [-0.4684,  0.2252, -0.8067,  ..., -0.6629,  0.1099,  0.4471],\n",
       "        [-0.2161, -0.2360, -0.9748,  ..., -0.8464, -0.6152,  0.4613],\n",
       "        [-0.7951,  0.9890,  0.5224,  ...,  0.9468,  0.6295,  0.6839]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # using -1 lets pytorch infer what the value should be (based on the og size)\n",
    "h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final layer\n",
    "W2 = torch.randn((100, 27))\n",
    "b2 = torch.randn(27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = h @ W2 + b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = logits.exp() # get fake counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = counts / counts.sum(1, keepdim=True) # normalize into a probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "shape mismatch: indexing tensors could not be broadcast together with shapes [32], [228146]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# comparing to label (expected character)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mprob[torch\u001b[39m.\u001b[39;49marange(\u001b[39m32\u001b[39;49m), Y]\u001b[39m.\u001b[39mlog()\u001b[39m.\u001b[39mmean()\n\u001b[1;32m      3\u001b[0m loss\n",
      "\u001b[0;31mIndexError\u001b[0m: shape mismatch: indexing tensors could not be broadcast together with shapes [32], [228146]"
     ]
    }
   ],
   "source": [
    "# comparing to label (expected character)\n",
    "loss = -prob[torch.arange(32), Y].log().mean()\n",
    "loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting this together\n",
    "Now, we're going to take everything we made above and make it look more organized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([228146, 3]), torch.Size([228146]))"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape # dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building parameters using a generator\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C = torch.randn((27, 2), generator=g)\n",
    "W1 = torch.randn((6,100), generator=g)\n",
    "b1 = torch.randn(100, generator=g)\n",
    "W2 = torch.randn((100, 27), generator=g)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "parameters = [C, W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3481"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# counting number of parameters\n",
    "sum(p.nelement() for p in parameters) # number of parameters in total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to require gradients for every parameter in the neural network. By default it is off becuase Tensor takes care of gradient when using their neural network. Since we are buidling our own neural network, we need to turn this on to get the gradient for each parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.556304931640625\n",
      "15.856295585632324\n",
      "12.287732124328613\n",
      "11.347538948059082\n",
      "15.4568510055542\n",
      "11.158556938171387\n",
      "12.20604133605957\n",
      "13.361774444580078\n",
      "10.342698097229004\n",
      "9.303770065307617\n",
      "10.26659870147705\n",
      "15.275646209716797\n",
      "8.97019100189209\n",
      "11.23560905456543\n",
      "8.488992691040039\n",
      "10.541656494140625\n",
      "8.632801055908203\n",
      "7.550400733947754\n",
      "9.809893608093262\n",
      "12.434541702270508\n",
      "13.016610145568848\n",
      "11.697427749633789\n",
      "9.402116775512695\n",
      "11.1528902053833\n",
      "13.054740905761719\n",
      "8.816349029541016\n",
      "12.807260513305664\n",
      "12.04638385772705\n",
      "11.644514083862305\n",
      "11.631548881530762\n",
      "8.438801765441895\n",
      "8.171278953552246\n",
      "8.938568115234375\n",
      "13.060914993286133\n",
      "13.514371871948242\n",
      "10.73193645477295\n",
      "10.064732551574707\n",
      "8.674356460571289\n",
      "9.12108039855957\n",
      "6.694490432739258\n",
      "8.400970458984375\n",
      "8.771952629089355\n",
      "10.345026969909668\n",
      "9.66441535949707\n",
      "5.981971740722656\n",
      "9.76171875\n",
      "10.438935279846191\n",
      "8.985917091369629\n",
      "8.054986000061035\n",
      "9.683183670043945\n",
      "9.729411125183105\n",
      "7.934333324432373\n",
      "9.02791690826416\n",
      "8.011935234069824\n",
      "8.677255630493164\n",
      "9.557597160339355\n",
      "9.985745429992676\n",
      "10.62968921661377\n",
      "8.53594970703125\n",
      "8.96415901184082\n",
      "11.480605125427246\n",
      "7.425069808959961\n",
      "9.913005828857422\n",
      "6.0022478103637695\n",
      "7.1380791664123535\n",
      "9.19160270690918\n",
      "7.555344104766846\n",
      "8.874650001525879\n",
      "10.372721672058105\n",
      "4.472775936126709\n",
      "7.719238758087158\n",
      "7.923393726348877\n",
      "7.376569747924805\n",
      "6.20586633682251\n",
      "10.719467163085938\n",
      "10.077178955078125\n",
      "8.356649398803711\n",
      "8.9749174118042\n",
      "7.716224670410156\n",
      "7.7576212882995605\n",
      "8.600533485412598\n",
      "7.441200256347656\n",
      "6.511050224304199\n",
      "5.385087013244629\n",
      "6.327981948852539\n",
      "9.025111198425293\n",
      "6.58053731918335\n",
      "7.889584064483643\n",
      "5.662131309509277\n",
      "7.211521148681641\n",
      "6.42440128326416\n",
      "5.48495626449585\n",
      "5.726150035858154\n",
      "7.45442008972168\n",
      "8.067185401916504\n",
      "7.453123569488525\n",
      "5.544681072235107\n",
      "5.541660785675049\n",
      "5.838562965393066\n",
      "8.198328018188477\n"
     ]
    }
   ],
   "source": [
    "for _ in range(100):\n",
    "    \n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, X.shape[0], (32,)) # creating a minibatch of size 32\n",
    "    \n",
    "    # calculating loss (forward pass)\n",
    "    emb = C[X[ix]] # (32, 3, 2)\n",
    "    h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # (32, 100)\n",
    "    logits = h @ W2 + b2 # (32, 27)\n",
    "    # counts = logits.exp()\n",
    "    # prob = counts / counts.sum(1, keepdim=True)\n",
    "    # loss = -prob[torch.arange(32), Y].log().mean()\n",
    "\n",
    "    # we can replace the above lines with a more condensed and efficient version\n",
    "    # it doesnt create new tensors like we do above but uses fused kernels for efficiency\n",
    "    # this also makes the backward pass more efficient bc the formulas can be simplified\n",
    "    loss = F.cross_entropy(logits, Y[ix])\n",
    "    print(loss.item())  \n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    # update parameters\n",
    "    for p in parameters:\n",
    "        p.data += -1 * p.grad # manual value is learning rate\n",
    "# print(loss.item())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.5732, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[X]\n",
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1)\n",
    "logits = h @ W2 + b2\n",
    "loss = F.cross_entropy(logits, Y)\n",
    "loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another problem with our original approach is that with very large values in our logits (which may happen during optimization), we run out of room in our float and we get 'inf' which becomes 'nan' when calcualting probs -- very very bad :(. Pytorch solved this by realizing that you could offset logits by any value and get the same probs result. Since negative values are ok but positive is bad, pytorch subtracts the largets value in the tensor from the logits. This makes the largest value in logits 0 and every other value will be negative. Yay no more problems (well behaved)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing training\n",
    "In reality, we don't train over the entire dataset every single time. Instead, we will do forward, backward and update passes over mini batches of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([139882, 119120, 223494,  29271,   3568, 164331,  79949,  68703, 112697,\n",
       "        113944,  27925, 198053,  57313,  19868, 181198,  97608,  43143, 216098,\n",
       "         82156, 121624,  75398,  89778,  34704, 117608, 150266, 124115, 165396,\n",
       "         51251, 126151,  50540,  14701,   5227])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randint(0, X.shape[0], (32,)) # creating a minibatch of size 32"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate\n",
    "How do we determine the learning rate? \n",
    "\n",
    "__STOPPED AT 45:52!!!!!!!!!__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found (through manual testing) that the optimal learning rate is somewhere between 0.001 and 1. To test different learning spaces were going to use linspace. This will result in the possible learning rates which we can search over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0010, 0.0020, 0.0030, 0.0040, 0.0050, 0.0060, 0.0070, 0.0080, 0.0090,\n",
       "        0.0100, 0.0110, 0.0120, 0.0130, 0.0140, 0.0150, 0.0160, 0.0170, 0.0180,\n",
       "        0.0190, 0.0200, 0.0210, 0.0220, 0.0230, 0.0240, 0.0250, 0.0260, 0.0270,\n",
       "        0.0280, 0.0290, 0.0300, 0.0310, 0.0320, 0.0330, 0.0340, 0.0350, 0.0360,\n",
       "        0.0370, 0.0380, 0.0390, 0.0400, 0.0410, 0.0420, 0.0430, 0.0440, 0.0450,\n",
       "        0.0460, 0.0470, 0.0480, 0.0490, 0.0500, 0.0510, 0.0520, 0.0530, 0.0540,\n",
       "        0.0550, 0.0560, 0.0570, 0.0580, 0.0590, 0.0600, 0.0610, 0.0620, 0.0630,\n",
       "        0.0640, 0.0650, 0.0660, 0.0670, 0.0680, 0.0690, 0.0700, 0.0710, 0.0720,\n",
       "        0.0730, 0.0740, 0.0750, 0.0760, 0.0770, 0.0780, 0.0790, 0.0800, 0.0810,\n",
       "        0.0820, 0.0830, 0.0840, 0.0850, 0.0860, 0.0870, 0.0880, 0.0890, 0.0900,\n",
       "        0.0910, 0.0920, 0.0930, 0.0940, 0.0950, 0.0960, 0.0970, 0.0980, 0.0990,\n",
       "        0.1000, 0.1010, 0.1020, 0.1030, 0.1040, 0.1050, 0.1060, 0.1070, 0.1080,\n",
       "        0.1090, 0.1100, 0.1110, 0.1120, 0.1130, 0.1140, 0.1150, 0.1160, 0.1170,\n",
       "        0.1180, 0.1190, 0.1200, 0.1210, 0.1220, 0.1230, 0.1240, 0.1250, 0.1260,\n",
       "        0.1270, 0.1280, 0.1290, 0.1300, 0.1310, 0.1320, 0.1330, 0.1340, 0.1350,\n",
       "        0.1360, 0.1370, 0.1380, 0.1390, 0.1400, 0.1410, 0.1420, 0.1430, 0.1440,\n",
       "        0.1450, 0.1460, 0.1470, 0.1480, 0.1490, 0.1500, 0.1510, 0.1520, 0.1530,\n",
       "        0.1540, 0.1550, 0.1560, 0.1570, 0.1580, 0.1590, 0.1600, 0.1610, 0.1620,\n",
       "        0.1630, 0.1640, 0.1650, 0.1660, 0.1670, 0.1680, 0.1690, 0.1700, 0.1710,\n",
       "        0.1720, 0.1730, 0.1740, 0.1750, 0.1760, 0.1770, 0.1780, 0.1790, 0.1800,\n",
       "        0.1810, 0.1820, 0.1830, 0.1840, 0.1850, 0.1860, 0.1870, 0.1880, 0.1890,\n",
       "        0.1900, 0.1910, 0.1920, 0.1930, 0.1940, 0.1950, 0.1960, 0.1970, 0.1980,\n",
       "        0.1990, 0.2000, 0.2010, 0.2020, 0.2030, 0.2040, 0.2050, 0.2060, 0.2070,\n",
       "        0.2080, 0.2090, 0.2100, 0.2110, 0.2120, 0.2130, 0.2140, 0.2150, 0.2160,\n",
       "        0.2170, 0.2180, 0.2190, 0.2200, 0.2210, 0.2220, 0.2230, 0.2240, 0.2250,\n",
       "        0.2260, 0.2270, 0.2280, 0.2290, 0.2300, 0.2310, 0.2320, 0.2330, 0.2340,\n",
       "        0.2350, 0.2360, 0.2370, 0.2380, 0.2390, 0.2400, 0.2410, 0.2420, 0.2430,\n",
       "        0.2440, 0.2450, 0.2460, 0.2470, 0.2480, 0.2490, 0.2500, 0.2510, 0.2520,\n",
       "        0.2530, 0.2540, 0.2550, 0.2560, 0.2570, 0.2580, 0.2590, 0.2600, 0.2610,\n",
       "        0.2620, 0.2630, 0.2640, 0.2650, 0.2660, 0.2670, 0.2680, 0.2690, 0.2700,\n",
       "        0.2710, 0.2720, 0.2730, 0.2740, 0.2750, 0.2760, 0.2770, 0.2780, 0.2790,\n",
       "        0.2800, 0.2810, 0.2820, 0.2830, 0.2840, 0.2850, 0.2860, 0.2870, 0.2880,\n",
       "        0.2890, 0.2900, 0.2910, 0.2920, 0.2930, 0.2940, 0.2950, 0.2960, 0.2970,\n",
       "        0.2980, 0.2990, 0.3000, 0.3010, 0.3020, 0.3030, 0.3040, 0.3050, 0.3060,\n",
       "        0.3070, 0.3080, 0.3090, 0.3100, 0.3110, 0.3120, 0.3130, 0.3140, 0.3150,\n",
       "        0.3160, 0.3170, 0.3180, 0.3190, 0.3200, 0.3210, 0.3220, 0.3230, 0.3240,\n",
       "        0.3250, 0.3260, 0.3270, 0.3280, 0.3290, 0.3300, 0.3310, 0.3320, 0.3330,\n",
       "        0.3340, 0.3350, 0.3360, 0.3370, 0.3380, 0.3390, 0.3400, 0.3410, 0.3420,\n",
       "        0.3430, 0.3440, 0.3450, 0.3460, 0.3470, 0.3480, 0.3490, 0.3500, 0.3510,\n",
       "        0.3520, 0.3530, 0.3540, 0.3550, 0.3560, 0.3570, 0.3580, 0.3590, 0.3600,\n",
       "        0.3610, 0.3620, 0.3630, 0.3640, 0.3650, 0.3660, 0.3670, 0.3680, 0.3690,\n",
       "        0.3700, 0.3710, 0.3720, 0.3730, 0.3740, 0.3750, 0.3760, 0.3770, 0.3780,\n",
       "        0.3790, 0.3800, 0.3810, 0.3820, 0.3830, 0.3840, 0.3850, 0.3860, 0.3870,\n",
       "        0.3880, 0.3890, 0.3900, 0.3910, 0.3920, 0.3930, 0.3940, 0.3950, 0.3960,\n",
       "        0.3970, 0.3980, 0.3990, 0.4000, 0.4010, 0.4020, 0.4030, 0.4040, 0.4050,\n",
       "        0.4060, 0.4070, 0.4080, 0.4090, 0.4100, 0.4110, 0.4120, 0.4130, 0.4140,\n",
       "        0.4150, 0.4160, 0.4170, 0.4180, 0.4190, 0.4200, 0.4210, 0.4220, 0.4230,\n",
       "        0.4240, 0.4250, 0.4260, 0.4270, 0.4280, 0.4290, 0.4300, 0.4310, 0.4320,\n",
       "        0.4330, 0.4340, 0.4350, 0.4360, 0.4370, 0.4380, 0.4390, 0.4400, 0.4410,\n",
       "        0.4420, 0.4430, 0.4440, 0.4450, 0.4460, 0.4470, 0.4480, 0.4490, 0.4500,\n",
       "        0.4510, 0.4520, 0.4530, 0.4540, 0.4550, 0.4560, 0.4570, 0.4580, 0.4590,\n",
       "        0.4600, 0.4610, 0.4620, 0.4630, 0.4640, 0.4650, 0.4660, 0.4670, 0.4680,\n",
       "        0.4690, 0.4700, 0.4710, 0.4720, 0.4730, 0.4740, 0.4750, 0.4760, 0.4770,\n",
       "        0.4780, 0.4790, 0.4800, 0.4810, 0.4820, 0.4830, 0.4840, 0.4850, 0.4860,\n",
       "        0.4870, 0.4880, 0.4890, 0.4900, 0.4910, 0.4920, 0.4930, 0.4940, 0.4950,\n",
       "        0.4960, 0.4970, 0.4980, 0.4990, 0.5000, 0.5010, 0.5020, 0.5030, 0.5040,\n",
       "        0.5050, 0.5060, 0.5070, 0.5080, 0.5090, 0.5100, 0.5110, 0.5120, 0.5130,\n",
       "        0.5140, 0.5150, 0.5160, 0.5170, 0.5180, 0.5190, 0.5200, 0.5210, 0.5220,\n",
       "        0.5230, 0.5240, 0.5250, 0.5260, 0.5270, 0.5280, 0.5290, 0.5300, 0.5310,\n",
       "        0.5320, 0.5330, 0.5340, 0.5350, 0.5360, 0.5370, 0.5380, 0.5390, 0.5400,\n",
       "        0.5410, 0.5420, 0.5430, 0.5440, 0.5450, 0.5460, 0.5470, 0.5480, 0.5490,\n",
       "        0.5500, 0.5510, 0.5520, 0.5530, 0.5540, 0.5550, 0.5560, 0.5570, 0.5580,\n",
       "        0.5590, 0.5600, 0.5610, 0.5620, 0.5630, 0.5640, 0.5650, 0.5660, 0.5670,\n",
       "        0.5680, 0.5690, 0.5700, 0.5710, 0.5720, 0.5730, 0.5740, 0.5750, 0.5760,\n",
       "        0.5770, 0.5780, 0.5790, 0.5800, 0.5810, 0.5820, 0.5830, 0.5840, 0.5850,\n",
       "        0.5860, 0.5870, 0.5880, 0.5890, 0.5900, 0.5910, 0.5920, 0.5930, 0.5940,\n",
       "        0.5950, 0.5960, 0.5970, 0.5980, 0.5990, 0.6000, 0.6010, 0.6020, 0.6030,\n",
       "        0.6040, 0.6050, 0.6060, 0.6070, 0.6080, 0.6090, 0.6100, 0.6110, 0.6120,\n",
       "        0.6130, 0.6140, 0.6150, 0.6160, 0.6170, 0.6180, 0.6190, 0.6200, 0.6210,\n",
       "        0.6220, 0.6230, 0.6240, 0.6250, 0.6260, 0.6270, 0.6280, 0.6290, 0.6300,\n",
       "        0.6310, 0.6320, 0.6330, 0.6340, 0.6350, 0.6360, 0.6370, 0.6380, 0.6390,\n",
       "        0.6400, 0.6410, 0.6420, 0.6430, 0.6440, 0.6450, 0.6460, 0.6470, 0.6480,\n",
       "        0.6490, 0.6500, 0.6510, 0.6520, 0.6530, 0.6540, 0.6550, 0.6560, 0.6570,\n",
       "        0.6580, 0.6590, 0.6600, 0.6610, 0.6620, 0.6630, 0.6640, 0.6650, 0.6660,\n",
       "        0.6670, 0.6680, 0.6690, 0.6700, 0.6710, 0.6720, 0.6730, 0.6740, 0.6750,\n",
       "        0.6760, 0.6770, 0.6780, 0.6790, 0.6800, 0.6810, 0.6820, 0.6830, 0.6840,\n",
       "        0.6850, 0.6860, 0.6870, 0.6880, 0.6890, 0.6900, 0.6910, 0.6920, 0.6930,\n",
       "        0.6940, 0.6950, 0.6960, 0.6970, 0.6980, 0.6990, 0.7000, 0.7010, 0.7020,\n",
       "        0.7030, 0.7040, 0.7050, 0.7060, 0.7070, 0.7080, 0.7090, 0.7100, 0.7110,\n",
       "        0.7120, 0.7130, 0.7140, 0.7150, 0.7160, 0.7170, 0.7180, 0.7190, 0.7200,\n",
       "        0.7210, 0.7220, 0.7230, 0.7240, 0.7250, 0.7260, 0.7270, 0.7280, 0.7290,\n",
       "        0.7300, 0.7310, 0.7320, 0.7330, 0.7340, 0.7350, 0.7360, 0.7370, 0.7380,\n",
       "        0.7390, 0.7400, 0.7410, 0.7420, 0.7430, 0.7440, 0.7450, 0.7460, 0.7470,\n",
       "        0.7480, 0.7490, 0.7500, 0.7510, 0.7520, 0.7530, 0.7540, 0.7550, 0.7560,\n",
       "        0.7570, 0.7580, 0.7590, 0.7600, 0.7610, 0.7620, 0.7630, 0.7640, 0.7650,\n",
       "        0.7660, 0.7670, 0.7680, 0.7690, 0.7700, 0.7710, 0.7720, 0.7730, 0.7740,\n",
       "        0.7750, 0.7760, 0.7770, 0.7780, 0.7790, 0.7800, 0.7810, 0.7820, 0.7830,\n",
       "        0.7840, 0.7850, 0.7860, 0.7870, 0.7880, 0.7890, 0.7900, 0.7910, 0.7920,\n",
       "        0.7930, 0.7940, 0.7950, 0.7960, 0.7970, 0.7980, 0.7990, 0.8000, 0.8010,\n",
       "        0.8020, 0.8030, 0.8040, 0.8050, 0.8060, 0.8070, 0.8080, 0.8090, 0.8100,\n",
       "        0.8110, 0.8120, 0.8130, 0.8140, 0.8150, 0.8160, 0.8170, 0.8180, 0.8190,\n",
       "        0.8200, 0.8210, 0.8220, 0.8230, 0.8240, 0.8250, 0.8260, 0.8270, 0.8280,\n",
       "        0.8290, 0.8300, 0.8310, 0.8320, 0.8330, 0.8340, 0.8350, 0.8360, 0.8370,\n",
       "        0.8380, 0.8390, 0.8400, 0.8410, 0.8420, 0.8430, 0.8440, 0.8450, 0.8460,\n",
       "        0.8470, 0.8480, 0.8490, 0.8500, 0.8510, 0.8520, 0.8530, 0.8540, 0.8550,\n",
       "        0.8560, 0.8570, 0.8580, 0.8590, 0.8600, 0.8610, 0.8620, 0.8630, 0.8640,\n",
       "        0.8650, 0.8660, 0.8670, 0.8680, 0.8690, 0.8700, 0.8710, 0.8720, 0.8730,\n",
       "        0.8740, 0.8750, 0.8760, 0.8770, 0.8780, 0.8790, 0.8800, 0.8810, 0.8820,\n",
       "        0.8830, 0.8840, 0.8850, 0.8860, 0.8870, 0.8880, 0.8890, 0.8900, 0.8910,\n",
       "        0.8920, 0.8930, 0.8940, 0.8950, 0.8960, 0.8970, 0.8980, 0.8990, 0.9000,\n",
       "        0.9010, 0.9020, 0.9030, 0.9040, 0.9050, 0.9060, 0.9070, 0.9080, 0.9090,\n",
       "        0.9100, 0.9110, 0.9120, 0.9130, 0.9140, 0.9150, 0.9160, 0.9170, 0.9180,\n",
       "        0.9190, 0.9200, 0.9210, 0.9220, 0.9230, 0.9240, 0.9250, 0.9260, 0.9270,\n",
       "        0.9280, 0.9290, 0.9300, 0.9310, 0.9320, 0.9330, 0.9340, 0.9350, 0.9360,\n",
       "        0.9370, 0.9380, 0.9390, 0.9400, 0.9410, 0.9420, 0.9430, 0.9440, 0.9450,\n",
       "        0.9460, 0.9470, 0.9480, 0.9490, 0.9500, 0.9510, 0.9520, 0.9530, 0.9540,\n",
       "        0.9550, 0.9560, 0.9570, 0.9580, 0.9590, 0.9600, 0.9610, 0.9620, 0.9630,\n",
       "        0.9640, 0.9650, 0.9660, 0.9670, 0.9680, 0.9690, 0.9700, 0.9710, 0.9720,\n",
       "        0.9730, 0.9740, 0.9750, 0.9760, 0.9770, 0.9780, 0.9790, 0.9800, 0.9810,\n",
       "        0.9820, 0.9830, 0.9840, 0.9850, 0.9860, 0.9870, 0.9880, 0.9890, 0.9900,\n",
       "        0.9910, 0.9920, 0.9930, 0.9940, 0.9950, 0.9960, 0.9970, 0.9980, 0.9990,\n",
       "        1.0000])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linspace(0.001, 1, 1000) # creates a tensor of 1000 values between 0.001 and 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This won't work well because we won't be spaced between each value, so instead we will use an exponential version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0011,\n",
       "        0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011,\n",
       "        0.0011, 0.0011, 0.0011, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012,\n",
       "        0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0013, 0.0013, 0.0013,\n",
       "        0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0014,\n",
       "        0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014,\n",
       "        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n",
       "        0.0015, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016,\n",
       "        0.0016, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017,\n",
       "        0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0019,\n",
       "        0.0019, 0.0019, 0.0019, 0.0019, 0.0019, 0.0019, 0.0019, 0.0020, 0.0020,\n",
       "        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0021, 0.0021, 0.0021, 0.0021,\n",
       "        0.0021, 0.0021, 0.0021, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "        0.0022, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0024, 0.0024,\n",
       "        0.0024, 0.0024, 0.0024, 0.0024, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
       "        0.0025, 0.0026, 0.0026, 0.0026, 0.0026, 0.0026, 0.0027, 0.0027, 0.0027,\n",
       "        0.0027, 0.0027, 0.0027, 0.0028, 0.0028, 0.0028, 0.0028, 0.0028, 0.0029,\n",
       "        0.0029, 0.0029, 0.0029, 0.0029, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030,\n",
       "        0.0031, 0.0031, 0.0031, 0.0031, 0.0032, 0.0032, 0.0032, 0.0032, 0.0032,\n",
       "        0.0033, 0.0033, 0.0033, 0.0033, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034,\n",
       "        0.0035, 0.0035, 0.0035, 0.0035, 0.0036, 0.0036, 0.0036, 0.0036, 0.0037,\n",
       "        0.0037, 0.0037, 0.0037, 0.0038, 0.0038, 0.0038, 0.0039, 0.0039, 0.0039,\n",
       "        0.0039, 0.0040, 0.0040, 0.0040, 0.0040, 0.0041, 0.0041, 0.0041, 0.0042,\n",
       "        0.0042, 0.0042, 0.0042, 0.0043, 0.0043, 0.0043, 0.0044, 0.0044, 0.0044,\n",
       "        0.0045, 0.0045, 0.0045, 0.0045, 0.0046, 0.0046, 0.0046, 0.0047, 0.0047,\n",
       "        0.0047, 0.0048, 0.0048, 0.0048, 0.0049, 0.0049, 0.0049, 0.0050, 0.0050,\n",
       "        0.0050, 0.0051, 0.0051, 0.0051, 0.0052, 0.0052, 0.0053, 0.0053, 0.0053,\n",
       "        0.0054, 0.0054, 0.0054, 0.0055, 0.0055, 0.0056, 0.0056, 0.0056, 0.0057,\n",
       "        0.0057, 0.0058, 0.0058, 0.0058, 0.0059, 0.0059, 0.0060, 0.0060, 0.0060,\n",
       "        0.0061, 0.0061, 0.0062, 0.0062, 0.0062, 0.0063, 0.0063, 0.0064, 0.0064,\n",
       "        0.0065, 0.0065, 0.0066, 0.0066, 0.0067, 0.0067, 0.0067, 0.0068, 0.0068,\n",
       "        0.0069, 0.0069, 0.0070, 0.0070, 0.0071, 0.0071, 0.0072, 0.0072, 0.0073,\n",
       "        0.0073, 0.0074, 0.0074, 0.0075, 0.0075, 0.0076, 0.0076, 0.0077, 0.0077,\n",
       "        0.0078, 0.0079, 0.0079, 0.0080, 0.0080, 0.0081, 0.0081, 0.0082, 0.0082,\n",
       "        0.0083, 0.0084, 0.0084, 0.0085, 0.0085, 0.0086, 0.0086, 0.0087, 0.0088,\n",
       "        0.0088, 0.0089, 0.0090, 0.0090, 0.0091, 0.0091, 0.0092, 0.0093, 0.0093,\n",
       "        0.0094, 0.0095, 0.0095, 0.0096, 0.0097, 0.0097, 0.0098, 0.0099, 0.0099,\n",
       "        0.0100, 0.0101, 0.0101, 0.0102, 0.0103, 0.0104, 0.0104, 0.0105, 0.0106,\n",
       "        0.0106, 0.0107, 0.0108, 0.0109, 0.0109, 0.0110, 0.0111, 0.0112, 0.0112,\n",
       "        0.0113, 0.0114, 0.0115, 0.0116, 0.0116, 0.0117, 0.0118, 0.0119, 0.0120,\n",
       "        0.0121, 0.0121, 0.0122, 0.0123, 0.0124, 0.0125, 0.0126, 0.0127, 0.0127,\n",
       "        0.0128, 0.0129, 0.0130, 0.0131, 0.0132, 0.0133, 0.0134, 0.0135, 0.0136,\n",
       "        0.0137, 0.0137, 0.0138, 0.0139, 0.0140, 0.0141, 0.0142, 0.0143, 0.0144,\n",
       "        0.0145, 0.0146, 0.0147, 0.0148, 0.0149, 0.0150, 0.0151, 0.0152, 0.0154,\n",
       "        0.0155, 0.0156, 0.0157, 0.0158, 0.0159, 0.0160, 0.0161, 0.0162, 0.0163,\n",
       "        0.0165, 0.0166, 0.0167, 0.0168, 0.0169, 0.0170, 0.0171, 0.0173, 0.0174,\n",
       "        0.0175, 0.0176, 0.0178, 0.0179, 0.0180, 0.0181, 0.0182, 0.0184, 0.0185,\n",
       "        0.0186, 0.0188, 0.0189, 0.0190, 0.0192, 0.0193, 0.0194, 0.0196, 0.0197,\n",
       "        0.0198, 0.0200, 0.0201, 0.0202, 0.0204, 0.0205, 0.0207, 0.0208, 0.0210,\n",
       "        0.0211, 0.0212, 0.0214, 0.0215, 0.0217, 0.0218, 0.0220, 0.0221, 0.0223,\n",
       "        0.0225, 0.0226, 0.0228, 0.0229, 0.0231, 0.0232, 0.0234, 0.0236, 0.0237,\n",
       "        0.0239, 0.0241, 0.0242, 0.0244, 0.0246, 0.0247, 0.0249, 0.0251, 0.0253,\n",
       "        0.0254, 0.0256, 0.0258, 0.0260, 0.0261, 0.0263, 0.0265, 0.0267, 0.0269,\n",
       "        0.0271, 0.0273, 0.0274, 0.0276, 0.0278, 0.0280, 0.0282, 0.0284, 0.0286,\n",
       "        0.0288, 0.0290, 0.0292, 0.0294, 0.0296, 0.0298, 0.0300, 0.0302, 0.0304,\n",
       "        0.0307, 0.0309, 0.0311, 0.0313, 0.0315, 0.0317, 0.0320, 0.0322, 0.0324,\n",
       "        0.0326, 0.0328, 0.0331, 0.0333, 0.0335, 0.0338, 0.0340, 0.0342, 0.0345,\n",
       "        0.0347, 0.0350, 0.0352, 0.0354, 0.0357, 0.0359, 0.0362, 0.0364, 0.0367,\n",
       "        0.0369, 0.0372, 0.0375, 0.0377, 0.0380, 0.0382, 0.0385, 0.0388, 0.0390,\n",
       "        0.0393, 0.0396, 0.0399, 0.0401, 0.0404, 0.0407, 0.0410, 0.0413, 0.0416,\n",
       "        0.0418, 0.0421, 0.0424, 0.0427, 0.0430, 0.0433, 0.0436, 0.0439, 0.0442,\n",
       "        0.0445, 0.0448, 0.0451, 0.0455, 0.0458, 0.0461, 0.0464, 0.0467, 0.0471,\n",
       "        0.0474, 0.0477, 0.0480, 0.0484, 0.0487, 0.0491, 0.0494, 0.0497, 0.0501,\n",
       "        0.0504, 0.0508, 0.0511, 0.0515, 0.0518, 0.0522, 0.0526, 0.0529, 0.0533,\n",
       "        0.0537, 0.0540, 0.0544, 0.0548, 0.0552, 0.0556, 0.0559, 0.0563, 0.0567,\n",
       "        0.0571, 0.0575, 0.0579, 0.0583, 0.0587, 0.0591, 0.0595, 0.0599, 0.0604,\n",
       "        0.0608, 0.0612, 0.0616, 0.0621, 0.0625, 0.0629, 0.0634, 0.0638, 0.0642,\n",
       "        0.0647, 0.0651, 0.0656, 0.0660, 0.0665, 0.0670, 0.0674, 0.0679, 0.0684,\n",
       "        0.0688, 0.0693, 0.0698, 0.0703, 0.0708, 0.0713, 0.0718, 0.0723, 0.0728,\n",
       "        0.0733, 0.0738, 0.0743, 0.0748, 0.0753, 0.0758, 0.0764, 0.0769, 0.0774,\n",
       "        0.0780, 0.0785, 0.0790, 0.0796, 0.0802, 0.0807, 0.0813, 0.0818, 0.0824,\n",
       "        0.0830, 0.0835, 0.0841, 0.0847, 0.0853, 0.0859, 0.0865, 0.0871, 0.0877,\n",
       "        0.0883, 0.0889, 0.0895, 0.0901, 0.0908, 0.0914, 0.0920, 0.0927, 0.0933,\n",
       "        0.0940, 0.0946, 0.0953, 0.0959, 0.0966, 0.0973, 0.0979, 0.0986, 0.0993,\n",
       "        0.1000, 0.1007, 0.1014, 0.1021, 0.1028, 0.1035, 0.1042, 0.1050, 0.1057,\n",
       "        0.1064, 0.1072, 0.1079, 0.1087, 0.1094, 0.1102, 0.1109, 0.1117, 0.1125,\n",
       "        0.1133, 0.1140, 0.1148, 0.1156, 0.1164, 0.1172, 0.1181, 0.1189, 0.1197,\n",
       "        0.1205, 0.1214, 0.1222, 0.1231, 0.1239, 0.1248, 0.1256, 0.1265, 0.1274,\n",
       "        0.1283, 0.1292, 0.1301, 0.1310, 0.1319, 0.1328, 0.1337, 0.1346, 0.1356,\n",
       "        0.1365, 0.1374, 0.1384, 0.1394, 0.1403, 0.1413, 0.1423, 0.1433, 0.1443,\n",
       "        0.1453, 0.1463, 0.1473, 0.1483, 0.1493, 0.1504, 0.1514, 0.1525, 0.1535,\n",
       "        0.1546, 0.1557, 0.1567, 0.1578, 0.1589, 0.1600, 0.1611, 0.1623, 0.1634,\n",
       "        0.1645, 0.1657, 0.1668, 0.1680, 0.1691, 0.1703, 0.1715, 0.1727, 0.1739,\n",
       "        0.1751, 0.1763, 0.1775, 0.1788, 0.1800, 0.1812, 0.1825, 0.1838, 0.1850,\n",
       "        0.1863, 0.1876, 0.1889, 0.1902, 0.1916, 0.1929, 0.1942, 0.1956, 0.1969,\n",
       "        0.1983, 0.1997, 0.2010, 0.2024, 0.2038, 0.2053, 0.2067, 0.2081, 0.2096,\n",
       "        0.2110, 0.2125, 0.2140, 0.2154, 0.2169, 0.2184, 0.2200, 0.2215, 0.2230,\n",
       "        0.2246, 0.2261, 0.2277, 0.2293, 0.2309, 0.2325, 0.2341, 0.2357, 0.2373,\n",
       "        0.2390, 0.2406, 0.2423, 0.2440, 0.2457, 0.2474, 0.2491, 0.2508, 0.2526,\n",
       "        0.2543, 0.2561, 0.2579, 0.2597, 0.2615, 0.2633, 0.2651, 0.2669, 0.2688,\n",
       "        0.2707, 0.2725, 0.2744, 0.2763, 0.2783, 0.2802, 0.2821, 0.2841, 0.2861,\n",
       "        0.2880, 0.2900, 0.2921, 0.2941, 0.2961, 0.2982, 0.3002, 0.3023, 0.3044,\n",
       "        0.3065, 0.3087, 0.3108, 0.3130, 0.3151, 0.3173, 0.3195, 0.3217, 0.3240,\n",
       "        0.3262, 0.3285, 0.3308, 0.3331, 0.3354, 0.3377, 0.3400, 0.3424, 0.3448,\n",
       "        0.3472, 0.3496, 0.3520, 0.3544, 0.3569, 0.3594, 0.3619, 0.3644, 0.3669,\n",
       "        0.3695, 0.3720, 0.3746, 0.3772, 0.3798, 0.3825, 0.3851, 0.3878, 0.3905,\n",
       "        0.3932, 0.3959, 0.3987, 0.4014, 0.4042, 0.4070, 0.4098, 0.4127, 0.4155,\n",
       "        0.4184, 0.4213, 0.4243, 0.4272, 0.4302, 0.4331, 0.4362, 0.4392, 0.4422,\n",
       "        0.4453, 0.4484, 0.4515, 0.4546, 0.4578, 0.4610, 0.4642, 0.4674, 0.4706,\n",
       "        0.4739, 0.4772, 0.4805, 0.4838, 0.4872, 0.4906, 0.4940, 0.4974, 0.5008,\n",
       "        0.5043, 0.5078, 0.5113, 0.5149, 0.5185, 0.5221, 0.5257, 0.5293, 0.5330,\n",
       "        0.5367, 0.5404, 0.5442, 0.5479, 0.5517, 0.5556, 0.5594, 0.5633, 0.5672,\n",
       "        0.5712, 0.5751, 0.5791, 0.5831, 0.5872, 0.5913, 0.5954, 0.5995, 0.6036,\n",
       "        0.6078, 0.6120, 0.6163, 0.6206, 0.6249, 0.6292, 0.6336, 0.6380, 0.6424,\n",
       "        0.6469, 0.6513, 0.6559, 0.6604, 0.6650, 0.6696, 0.6743, 0.6789, 0.6837,\n",
       "        0.6884, 0.6932, 0.6980, 0.7028, 0.7077, 0.7126, 0.7176, 0.7225, 0.7275,\n",
       "        0.7326, 0.7377, 0.7428, 0.7480, 0.7531, 0.7584, 0.7636, 0.7689, 0.7743,\n",
       "        0.7796, 0.7850, 0.7905, 0.7960, 0.8015, 0.8071, 0.8127, 0.8183, 0.8240,\n",
       "        0.8297, 0.8355, 0.8412, 0.8471, 0.8530, 0.8589, 0.8648, 0.8708, 0.8769,\n",
       "        0.8830, 0.8891, 0.8953, 0.9015, 0.9077, 0.9140, 0.9204, 0.9268, 0.9332,\n",
       "        0.9397, 0.9462, 0.9528, 0.9594, 0.9660, 0.9727, 0.9795, 0.9863, 0.9931,\n",
       "        1.0000])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lre = torch.linspace(-3, 0, 1000)\n",
    "lrs = 10**lre\n",
    "lrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will include these learning rates in the training. This will mean that the learning rate will start as 0.001 and end at 1. Along the way we will track stats for each learning rate and its associated loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.483184814453125\n",
      "22.354785919189453\n",
      "20.80984115600586\n",
      "17.23212242126465\n",
      "17.62961196899414\n",
      "18.437232971191406\n",
      "21.97251319885254\n",
      "20.00156021118164\n",
      "23.7876033782959\n",
      "16.508405685424805\n",
      "20.313186645507812\n",
      "19.712778091430664\n",
      "17.695693969726562\n",
      "17.681489944458008\n",
      "20.660940170288086\n",
      "17.446788787841797\n",
      "20.983671188354492\n",
      "18.088838577270508\n",
      "17.385412216186523\n",
      "18.134172439575195\n",
      "19.71392250061035\n",
      "20.150527954101562\n",
      "17.924795150756836\n",
      "15.3855562210083\n",
      "18.966066360473633\n",
      "16.288278579711914\n",
      "17.840993881225586\n",
      "18.433530807495117\n",
      "17.428409576416016\n",
      "20.908248901367188\n",
      "16.58711051940918\n",
      "21.205387115478516\n",
      "19.61207389831543\n",
      "21.774906158447266\n",
      "14.176729202270508\n",
      "19.85826873779297\n",
      "17.191495895385742\n",
      "15.66231918334961\n",
      "19.170988082885742\n",
      "16.50863265991211\n",
      "16.77484893798828\n",
      "17.530668258666992\n",
      "19.951326370239258\n",
      "17.47499656677246\n",
      "15.604483604431152\n",
      "16.264934539794922\n",
      "19.246116638183594\n",
      "14.682537078857422\n",
      "20.819894790649414\n",
      "16.603818893432617\n",
      "18.29820442199707\n",
      "18.85133171081543\n",
      "16.762313842773438\n",
      "18.60185432434082\n",
      "15.55333137512207\n",
      "20.28217887878418\n",
      "16.54422378540039\n",
      "18.19675064086914\n",
      "17.525434494018555\n",
      "17.654436111450195\n",
      "16.93140411376953\n",
      "18.267574310302734\n",
      "18.54979133605957\n",
      "12.87429428100586\n",
      "15.64719009399414\n",
      "20.318620681762695\n",
      "18.234512329101562\n",
      "17.30353355407715\n",
      "17.90437126159668\n",
      "16.488563537597656\n",
      "15.256719589233398\n",
      "16.421939849853516\n",
      "14.34390640258789\n",
      "15.48703384399414\n",
      "18.04349136352539\n",
      "14.871076583862305\n",
      "16.4796085357666\n",
      "16.704666137695312\n",
      "19.952377319335938\n",
      "17.77577018737793\n",
      "16.949758529663086\n",
      "15.271379470825195\n",
      "15.037071228027344\n",
      "14.76927375793457\n",
      "17.692785263061523\n",
      "17.829683303833008\n",
      "17.729944229125977\n",
      "14.70880126953125\n",
      "17.612106323242188\n",
      "17.93181800842285\n",
      "19.42236328125\n",
      "18.02880096435547\n",
      "18.079858779907227\n",
      "16.058629989624023\n",
      "18.16849136352539\n",
      "12.781937599182129\n",
      "15.183573722839355\n",
      "17.910682678222656\n",
      "17.754728317260742\n",
      "17.58818817138672\n",
      "16.51680564880371\n",
      "15.499083518981934\n",
      "15.699141502380371\n",
      "17.80978775024414\n",
      "12.781243324279785\n",
      "19.307514190673828\n",
      "15.085504531860352\n",
      "16.894468307495117\n",
      "16.40765380859375\n",
      "15.817983627319336\n",
      "15.035381317138672\n",
      "14.080941200256348\n",
      "15.717788696289062\n",
      "17.7780704498291\n",
      "17.035085678100586\n",
      "15.284886360168457\n",
      "17.734947204589844\n",
      "17.65673065185547\n",
      "16.905643463134766\n",
      "16.292173385620117\n",
      "14.432735443115234\n",
      "13.604558944702148\n",
      "18.008106231689453\n",
      "16.552352905273438\n",
      "16.410871505737305\n",
      "16.544113159179688\n",
      "16.529577255249023\n",
      "14.350013732910156\n",
      "13.422999382019043\n",
      "15.845983505249023\n",
      "17.046049118041992\n",
      "16.518518447875977\n",
      "16.426240921020508\n",
      "18.612964630126953\n",
      "14.58950138092041\n",
      "17.35669708251953\n",
      "14.813026428222656\n",
      "17.914703369140625\n",
      "15.599014282226562\n",
      "16.959352493286133\n",
      "17.666854858398438\n",
      "18.205759048461914\n",
      "14.21641731262207\n",
      "16.369291305541992\n",
      "14.812820434570312\n",
      "15.917864799499512\n",
      "13.812625885009766\n",
      "17.236534118652344\n",
      "13.079803466796875\n",
      "16.07928466796875\n",
      "13.30374813079834\n",
      "13.330145835876465\n",
      "16.27208137512207\n",
      "15.459074020385742\n",
      "13.496721267700195\n",
      "12.50593090057373\n",
      "15.560482025146484\n",
      "12.274608612060547\n",
      "14.670112609863281\n",
      "17.134824752807617\n",
      "15.156792640686035\n",
      "16.591197967529297\n",
      "18.870635986328125\n",
      "12.804180145263672\n",
      "15.807509422302246\n",
      "14.030332565307617\n",
      "16.40681266784668\n",
      "14.188179016113281\n",
      "15.521679878234863\n",
      "12.770938873291016\n",
      "15.086365699768066\n",
      "15.72003173828125\n",
      "15.776905059814453\n",
      "15.251213073730469\n",
      "15.089151382446289\n",
      "14.841277122497559\n",
      "17.15029525756836\n",
      "18.2696475982666\n",
      "14.51874828338623\n",
      "15.660809516906738\n",
      "15.878332138061523\n",
      "13.086613655090332\n",
      "14.14067268371582\n",
      "14.576903343200684\n",
      "14.764666557312012\n",
      "12.174030303955078\n",
      "16.548479080200195\n",
      "12.091068267822266\n",
      "12.685468673706055\n",
      "11.962180137634277\n",
      "14.997949600219727\n",
      "14.487865447998047\n",
      "13.98635482788086\n",
      "13.64317512512207\n",
      "14.65217399597168\n",
      "17.0568904876709\n",
      "14.650565147399902\n",
      "14.892243385314941\n",
      "17.005678176879883\n",
      "15.631287574768066\n",
      "14.066082000732422\n",
      "15.270038604736328\n",
      "12.187816619873047\n",
      "14.12143325805664\n",
      "15.056239128112793\n",
      "14.280664443969727\n",
      "14.014244079589844\n",
      "13.295815467834473\n",
      "12.808344841003418\n",
      "15.463081359863281\n",
      "12.377182960510254\n",
      "12.44676685333252\n",
      "14.994017601013184\n",
      "14.41714859008789\n",
      "13.439414978027344\n",
      "11.322022438049316\n",
      "13.527843475341797\n",
      "11.971282005310059\n",
      "15.142589569091797\n",
      "14.269615173339844\n",
      "14.126602172851562\n",
      "11.823162078857422\n",
      "14.514613151550293\n",
      "12.711630821228027\n",
      "17.941030502319336\n",
      "13.133044242858887\n",
      "11.984644889831543\n",
      "11.978325843811035\n",
      "11.824417114257812\n",
      "9.39936351776123\n",
      "10.575093269348145\n",
      "11.677765846252441\n",
      "13.476668357849121\n",
      "11.811162948608398\n",
      "14.003732681274414\n",
      "11.51773452758789\n",
      "13.286178588867188\n",
      "14.74209976196289\n",
      "12.349257469177246\n",
      "11.657695770263672\n",
      "10.88580322265625\n",
      "11.642472267150879\n",
      "11.67902946472168\n",
      "14.443771362304688\n",
      "11.642777442932129\n",
      "12.490735054016113\n",
      "12.18267822265625\n",
      "11.099138259887695\n",
      "12.726947784423828\n",
      "12.193009376525879\n",
      "10.658915519714355\n",
      "11.822041511535645\n",
      "12.204536437988281\n",
      "12.37972354888916\n",
      "11.216512680053711\n",
      "13.5548677444458\n",
      "11.2781343460083\n",
      "10.832098960876465\n",
      "12.950186729431152\n",
      "13.202998161315918\n",
      "11.59149169921875\n",
      "12.822526931762695\n",
      "11.53264331817627\n",
      "11.35916519165039\n",
      "9.45665168762207\n",
      "11.360650062561035\n",
      "12.115522384643555\n",
      "9.760383605957031\n",
      "8.414894104003906\n",
      "11.499403953552246\n",
      "9.392485618591309\n",
      "10.213459014892578\n",
      "10.825641632080078\n",
      "10.852804183959961\n",
      "11.26248836517334\n",
      "11.33383846282959\n",
      "9.91239070892334\n",
      "11.710319519042969\n",
      "9.998489379882812\n",
      "10.477136611938477\n",
      "8.885592460632324\n",
      "12.222742080688477\n",
      "10.449592590332031\n",
      "13.241596221923828\n",
      "8.012722969055176\n",
      "12.144979476928711\n",
      "8.571044921875\n",
      "12.059502601623535\n",
      "10.018726348876953\n",
      "11.296794891357422\n",
      "12.535930633544922\n",
      "11.564557075500488\n",
      "10.292668342590332\n",
      "9.987953186035156\n",
      "15.495282173156738\n",
      "13.657389640808105\n",
      "11.427379608154297\n",
      "10.604194641113281\n",
      "10.103726387023926\n",
      "10.561660766601562\n",
      "12.82898235321045\n",
      "7.926311492919922\n",
      "11.326972007751465\n",
      "9.574848175048828\n",
      "10.850228309631348\n",
      "11.68592643737793\n",
      "12.83883285522461\n",
      "14.235557556152344\n",
      "10.337228775024414\n",
      "8.771526336669922\n",
      "10.593188285827637\n",
      "9.568286895751953\n",
      "10.977317810058594\n",
      "8.329296112060547\n",
      "11.512575149536133\n",
      "10.637174606323242\n",
      "12.220189094543457\n",
      "10.415579795837402\n",
      "10.547440528869629\n",
      "10.8054838180542\n",
      "12.052597045898438\n",
      "8.514998435974121\n",
      "9.472491264343262\n",
      "10.12066650390625\n",
      "9.134943962097168\n",
      "8.591409683227539\n",
      "10.706727981567383\n",
      "10.638053894042969\n",
      "10.753665924072266\n",
      "11.140933990478516\n",
      "11.369233131408691\n",
      "11.474123001098633\n",
      "11.28739070892334\n",
      "8.243548393249512\n",
      "7.259654998779297\n",
      "10.120931625366211\n",
      "10.131875038146973\n",
      "11.55787467956543\n",
      "9.673584938049316\n",
      "9.664220809936523\n",
      "8.519404411315918\n",
      "9.741592407226562\n",
      "8.22368049621582\n",
      "9.693387985229492\n",
      "7.73356819152832\n",
      "8.94903564453125\n",
      "12.026105880737305\n",
      "7.434138298034668\n",
      "8.584062576293945\n",
      "12.251203536987305\n",
      "10.11117172241211\n",
      "9.781632423400879\n",
      "10.282748222351074\n",
      "7.53637170791626\n",
      "9.039727210998535\n",
      "9.304559707641602\n",
      "9.03116512298584\n",
      "8.163588523864746\n",
      "9.312749862670898\n",
      "8.891314506530762\n",
      "8.166682243347168\n",
      "6.851129531860352\n",
      "9.969408988952637\n",
      "8.477276802062988\n",
      "8.062383651733398\n",
      "8.73334789276123\n",
      "8.673272132873535\n",
      "9.512187004089355\n",
      "6.821985721588135\n",
      "8.84992504119873\n",
      "9.135396003723145\n",
      "7.819611549377441\n",
      "8.567739486694336\n",
      "6.1981682777404785\n",
      "10.359844207763672\n",
      "10.061585426330566\n",
      "8.404706001281738\n",
      "9.46817398071289\n",
      "8.238908767700195\n",
      "8.706490516662598\n",
      "9.784212112426758\n",
      "8.842385292053223\n",
      "6.749496936798096\n",
      "11.084028244018555\n",
      "8.98116683959961\n",
      "8.240499496459961\n",
      "7.993485927581787\n",
      "8.612763404846191\n",
      "9.734094619750977\n",
      "7.750056266784668\n",
      "10.868253707885742\n",
      "9.916603088378906\n",
      "8.931729316711426\n",
      "7.066796779632568\n",
      "7.572661876678467\n",
      "7.636728763580322\n",
      "7.0598554611206055\n",
      "8.795339584350586\n",
      "7.403738021850586\n",
      "7.291914939880371\n",
      "6.952378273010254\n",
      "7.086188793182373\n",
      "7.674683570861816\n",
      "8.374988555908203\n",
      "7.577920436859131\n",
      "7.878326892852783\n",
      "8.221214294433594\n",
      "7.656599998474121\n",
      "8.759291648864746\n",
      "7.241633892059326\n",
      "6.476640701293945\n",
      "10.407498359680176\n",
      "7.473566055297852\n",
      "8.780359268188477\n",
      "9.598533630371094\n",
      "7.0292067527771\n",
      "8.50293254852295\n",
      "7.151240348815918\n",
      "7.692086696624756\n",
      "8.716787338256836\n",
      "6.9830427169799805\n",
      "8.37329387664795\n",
      "8.182628631591797\n",
      "6.703736305236816\n",
      "7.543623447418213\n",
      "6.786710739135742\n",
      "6.305300235748291\n",
      "5.944381237030029\n",
      "7.8159637451171875\n",
      "9.280014991760254\n",
      "8.75304889678955\n",
      "7.408867835998535\n",
      "9.230521202087402\n",
      "6.386584758758545\n",
      "7.792421340942383\n",
      "6.929275035858154\n",
      "7.847503662109375\n",
      "6.728895664215088\n",
      "6.2651824951171875\n",
      "7.0831427574157715\n",
      "8.841206550598145\n",
      "6.409605979919434\n",
      "5.878960609436035\n",
      "5.685690879821777\n",
      "7.446889877319336\n",
      "6.5515828132629395\n",
      "5.449671745300293\n",
      "5.132696628570557\n",
      "6.897715091705322\n",
      "5.472841262817383\n",
      "6.858504295349121\n",
      "7.730890274047852\n",
      "5.763260364532471\n",
      "5.867790222167969\n",
      "6.552321910858154\n",
      "6.2912092208862305\n",
      "8.559471130371094\n",
      "5.5641374588012695\n",
      "6.338924884796143\n",
      "5.923518180847168\n",
      "4.375969409942627\n",
      "7.149524211883545\n",
      "6.50312614440918\n",
      "4.054696559906006\n",
      "6.850225448608398\n",
      "5.931248664855957\n",
      "6.222606658935547\n",
      "6.245051860809326\n",
      "6.724433898925781\n",
      "5.266965866088867\n",
      "7.279106140136719\n",
      "6.4384636878967285\n",
      "7.516605377197266\n",
      "4.530974864959717\n",
      "5.656194686889648\n",
      "6.041436672210693\n",
      "6.615359306335449\n",
      "6.574132919311523\n",
      "5.6583333015441895\n",
      "5.403264999389648\n",
      "7.477911472320557\n",
      "6.23579216003418\n",
      "6.469816207885742\n",
      "6.403406143188477\n",
      "6.586163520812988\n",
      "5.258958339691162\n",
      "5.272604465484619\n",
      "5.959638595581055\n",
      "3.7732951641082764\n",
      "6.55106782913208\n",
      "6.9013237953186035\n",
      "7.461662292480469\n",
      "4.029541015625\n",
      "5.791253089904785\n",
      "4.443556308746338\n",
      "5.717834949493408\n",
      "6.59001350402832\n",
      "6.424333095550537\n",
      "4.832336902618408\n",
      "5.139641284942627\n",
      "6.492263317108154\n",
      "5.577683925628662\n",
      "4.272443771362305\n",
      "5.755636215209961\n",
      "5.103553771972656\n",
      "7.680496692657471\n",
      "4.4854302406311035\n",
      "5.12276554107666\n",
      "5.600521087646484\n",
      "5.468201637268066\n",
      "6.053813934326172\n",
      "6.409014701843262\n",
      "4.283052921295166\n",
      "4.426253795623779\n",
      "4.045802593231201\n",
      "5.5158820152282715\n",
      "6.306343078613281\n",
      "5.265575408935547\n",
      "5.184010982513428\n",
      "6.09982967376709\n",
      "5.304576873779297\n",
      "4.760325908660889\n",
      "4.963404655456543\n",
      "4.465228080749512\n",
      "5.85391902923584\n",
      "6.134868621826172\n",
      "5.743478298187256\n",
      "6.233476161956787\n",
      "5.828004837036133\n",
      "4.42830228805542\n",
      "4.675023078918457\n",
      "5.6441497802734375\n",
      "5.22576904296875\n",
      "4.647188663482666\n",
      "3.478790283203125\n",
      "5.7624735832214355\n",
      "4.44126033782959\n",
      "4.991338729858398\n",
      "5.292182922363281\n",
      "3.0651705265045166\n",
      "4.379488945007324\n",
      "5.838125705718994\n",
      "4.747922420501709\n",
      "4.26750373840332\n",
      "4.623346328735352\n",
      "4.585727691650391\n",
      "3.2598230838775635\n",
      "5.131969928741455\n",
      "5.591718673706055\n",
      "4.031804084777832\n",
      "5.3515424728393555\n",
      "4.450619697570801\n",
      "4.071571350097656\n",
      "4.779899597167969\n",
      "5.368561744689941\n",
      "5.050372123718262\n",
      "4.358776092529297\n",
      "5.394332408905029\n",
      "6.005086898803711\n",
      "4.337743759155273\n",
      "5.060194492340088\n",
      "3.557217836380005\n",
      "3.340362548828125\n",
      "4.754467964172363\n",
      "3.9869346618652344\n",
      "4.809444427490234\n",
      "4.423791885375977\n",
      "4.3625807762146\n",
      "4.858272075653076\n",
      "4.380346298217773\n",
      "4.371399402618408\n",
      "3.5812013149261475\n",
      "4.184558868408203\n",
      "3.3478426933288574\n",
      "3.4024858474731445\n",
      "4.819118499755859\n",
      "4.648220539093018\n",
      "4.504592418670654\n",
      "4.547188758850098\n",
      "3.3467845916748047\n",
      "3.724032402038574\n",
      "3.96203875541687\n",
      "4.5772809982299805\n",
      "4.335927486419678\n",
      "4.016330718994141\n",
      "4.63093376159668\n",
      "4.27779483795166\n",
      "3.9039530754089355\n",
      "3.433053493499756\n",
      "3.944516897201538\n",
      "4.515227317810059\n",
      "3.9782965183258057\n",
      "3.9623472690582275\n",
      "3.8227169513702393\n",
      "4.110306739807129\n",
      "4.769561290740967\n",
      "4.363842010498047\n",
      "3.650907039642334\n",
      "4.981672763824463\n",
      "4.292390823364258\n",
      "3.259999990463257\n",
      "3.881824254989624\n",
      "2.886760950088501\n",
      "3.7520663738250732\n",
      "3.2878804206848145\n",
      "4.126613616943359\n",
      "3.9644548892974854\n",
      "3.7922446727752686\n",
      "4.123460292816162\n",
      "3.989166498184204\n",
      "4.03957986831665\n",
      "3.222928762435913\n",
      "3.337611436843872\n",
      "4.216763019561768\n",
      "4.281750202178955\n",
      "3.976297616958618\n",
      "3.4891529083251953\n",
      "3.2055695056915283\n",
      "3.510740280151367\n",
      "3.6331498622894287\n",
      "3.232675552368164\n",
      "5.593516826629639\n",
      "3.815607786178589\n",
      "3.431182861328125\n",
      "3.684441566467285\n",
      "2.99456787109375\n",
      "3.170898199081421\n",
      "4.662319660186768\n",
      "3.4633917808532715\n",
      "2.7782983779907227\n",
      "3.770707368850708\n",
      "4.07474422454834\n",
      "3.142376661300659\n",
      "2.738097906112671\n",
      "3.2980217933654785\n",
      "3.3527889251708984\n",
      "2.966999053955078\n",
      "3.92415714263916\n",
      "3.181137800216675\n",
      "4.303863525390625\n",
      "2.9718916416168213\n",
      "2.7221148014068604\n",
      "3.0071160793304443\n",
      "3.130495309829712\n",
      "2.9502456188201904\n",
      "3.3335697650909424\n",
      "3.4706270694732666\n",
      "3.4616053104400635\n",
      "2.708083152770996\n",
      "3.4931857585906982\n",
      "3.0683727264404297\n",
      "3.071756601333618\n",
      "3.85190749168396\n",
      "3.5161259174346924\n",
      "3.2615249156951904\n",
      "3.53066086769104\n",
      "3.8988962173461914\n",
      "3.890859842300415\n",
      "4.011231899261475\n",
      "2.8795063495635986\n",
      "3.606642007827759\n",
      "2.6438472270965576\n",
      "3.209960699081421\n",
      "3.9614644050598145\n",
      "3.602180004119873\n",
      "3.10616135597229\n",
      "3.281099557876587\n",
      "3.997634172439575\n",
      "3.0557353496551514\n",
      "3.2762045860290527\n",
      "3.496880292892456\n",
      "3.2229456901550293\n",
      "2.8210458755493164\n",
      "3.3817718029022217\n",
      "3.098151922225952\n",
      "3.1398768424987793\n",
      "3.231872081756592\n",
      "3.41115140914917\n",
      "2.964925765991211\n",
      "3.2136178016662598\n",
      "3.3869788646698\n",
      "3.1080102920532227\n",
      "3.14255690574646\n",
      "3.334716796875\n",
      "3.085875988006592\n",
      "4.474527359008789\n",
      "3.2022056579589844\n",
      "4.366414546966553\n",
      "3.0801072120666504\n",
      "3.0699946880340576\n",
      "3.3556621074676514\n",
      "3.0024313926696777\n",
      "2.644252300262451\n",
      "3.477515459060669\n",
      "2.978043556213379\n",
      "3.2595114707946777\n",
      "3.6539065837860107\n",
      "3.724116563796997\n",
      "4.228198528289795\n",
      "2.782068967819214\n",
      "3.064361810684204\n",
      "2.8956074714660645\n",
      "2.84218692779541\n",
      "3.768913745880127\n",
      "2.833836317062378\n",
      "3.388780355453491\n",
      "3.1556341648101807\n",
      "3.2399990558624268\n",
      "3.3611948490142822\n",
      "3.2057175636291504\n",
      "2.937133550643921\n",
      "3.149303436279297\n",
      "2.8767669200897217\n",
      "2.9479546546936035\n",
      "2.800487518310547\n",
      "3.4200987815856934\n",
      "3.321464776992798\n",
      "3.305025339126587\n",
      "2.977114200592041\n",
      "3.2980542182922363\n",
      "2.9022929668426514\n",
      "3.0209665298461914\n",
      "3.301086902618408\n",
      "2.76029896736145\n",
      "3.0242226123809814\n",
      "2.7477991580963135\n",
      "3.0560555458068848\n",
      "3.223538398742676\n",
      "3.7433180809020996\n",
      "2.9196367263793945\n",
      "3.8786990642547607\n",
      "2.7045650482177734\n",
      "3.0648674964904785\n",
      "3.2423508167266846\n",
      "3.6497387886047363\n",
      "3.274589776992798\n",
      "3.536252021789551\n",
      "2.8497252464294434\n",
      "3.1367027759552\n",
      "3.7921767234802246\n",
      "3.361849069595337\n",
      "3.3490359783172607\n",
      "2.8240702152252197\n",
      "2.7164480686187744\n",
      "4.08732795715332\n",
      "2.703643321990967\n",
      "3.553791046142578\n",
      "2.696516513824463\n",
      "2.91719388961792\n",
      "3.3662118911743164\n",
      "2.9444169998168945\n",
      "2.8982975482940674\n",
      "2.6021249294281006\n",
      "2.5380005836486816\n",
      "3.754298210144043\n",
      "3.1377861499786377\n",
      "3.144819974899292\n",
      "3.569044351577759\n",
      "3.5137832164764404\n",
      "3.312295436859131\n",
      "2.509803295135498\n",
      "2.9486584663391113\n",
      "3.229597806930542\n",
      "3.6033802032470703\n",
      "2.9636647701263428\n",
      "3.2953104972839355\n",
      "3.3628180027008057\n",
      "2.8313636779785156\n",
      "4.269418716430664\n",
      "3.176276922225952\n",
      "3.4197351932525635\n",
      "2.8910202980041504\n",
      "3.637223958969116\n",
      "3.537903070449829\n",
      "3.595439910888672\n",
      "2.8388724327087402\n",
      "3.0190227031707764\n",
      "2.994126558303833\n",
      "3.256915330886841\n",
      "3.028642416000366\n",
      "3.333696126937866\n",
      "3.356642723083496\n",
      "2.927196502685547\n",
      "3.1814656257629395\n",
      "2.670470714569092\n",
      "3.3429341316223145\n",
      "3.421319007873535\n",
      "3.6407973766326904\n",
      "3.9626810550689697\n",
      "2.9040634632110596\n",
      "3.43192982673645\n",
      "3.580286741256714\n",
      "2.887681722640991\n",
      "2.9105677604675293\n",
      "2.9639766216278076\n",
      "3.1890623569488525\n",
      "4.105386257171631\n",
      "3.3531694412231445\n",
      "4.466113567352295\n",
      "3.880707263946533\n",
      "3.4926364421844482\n",
      "2.9947760105133057\n",
      "3.489690065383911\n",
      "3.0414516925811768\n",
      "4.140500068664551\n",
      "3.1814990043640137\n",
      "3.055690050125122\n",
      "3.0824429988861084\n",
      "3.077101707458496\n",
      "3.16328501701355\n",
      "3.031804084777832\n",
      "3.7075347900390625\n",
      "3.1573374271392822\n",
      "4.070926666259766\n",
      "4.011131286621094\n",
      "3.184324026107788\n",
      "3.5280308723449707\n",
      "2.9941604137420654\n",
      "3.3559412956237793\n",
      "2.107926368713379\n",
      "3.1849114894866943\n",
      "2.9813356399536133\n",
      "3.5849225521087646\n",
      "3.7873857021331787\n",
      "3.7611379623413086\n",
      "3.7388999462127686\n",
      "3.348508358001709\n",
      "3.0583956241607666\n",
      "3.6069891452789307\n",
      "3.614705801010132\n",
      "3.9098570346832275\n",
      "3.4456801414489746\n",
      "4.489173412322998\n",
      "3.3517632484436035\n",
      "3.804940700531006\n",
      "3.6268041133880615\n",
      "2.710439682006836\n",
      "3.280156373977661\n",
      "3.295201301574707\n",
      "3.369520664215088\n",
      "3.2394020557403564\n",
      "3.5035102367401123\n",
      "3.7245001792907715\n",
      "4.190086364746094\n",
      "3.8955912590026855\n",
      "4.2992024421691895\n",
      "4.2692975997924805\n",
      "3.740591526031494\n",
      "4.034459590911865\n",
      "3.541010618209839\n",
      "3.187709331512451\n",
      "3.09686541557312\n",
      "4.189955711364746\n",
      "3.7413525581359863\n",
      "3.8655924797058105\n",
      "3.7015256881713867\n",
      "3.4615142345428467\n",
      "6.1962666511535645\n",
      "5.0079240798950195\n",
      "3.6960394382476807\n",
      "3.774030923843384\n",
      "3.1403300762176514\n",
      "2.8698883056640625\n",
      "3.3470168113708496\n",
      "3.8594746589660645\n",
      "4.368743419647217\n",
      "4.0196404457092285\n",
      "3.9828879833221436\n",
      "2.925377607345581\n",
      "3.6122331619262695\n",
      "3.469287633895874\n",
      "3.6633992195129395\n",
      "4.507410526275635\n",
      "3.5513861179351807\n",
      "4.902601718902588\n",
      "3.7188212871551514\n",
      "4.730559825897217\n",
      "5.077823638916016\n",
      "6.422189712524414\n",
      "6.034177303314209\n",
      "5.038954734802246\n",
      "3.9674298763275146\n",
      "3.7446248531341553\n",
      "4.580067157745361\n",
      "4.306655406951904\n",
      "9.183683395385742\n",
      "8.223578453063965\n",
      "5.47765588760376\n",
      "7.167868137359619\n",
      "6.5195631980896\n",
      "5.246999740600586\n",
      "3.8662269115448\n",
      "5.204437732696533\n",
      "5.722408294677734\n",
      "5.167442798614502\n",
      "4.425186634063721\n",
      "3.9050209522247314\n",
      "4.289339542388916\n",
      "4.591614246368408\n",
      "4.4753241539001465\n",
      "6.007528305053711\n",
      "4.1765336990356445\n",
      "6.495079040527344\n",
      "4.606022357940674\n",
      "4.795463562011719\n",
      "4.633749961853027\n",
      "3.597320556640625\n",
      "5.288435459136963\n",
      "4.607229232788086\n",
      "4.7570648193359375\n",
      "3.8424155712127686\n",
      "5.00752067565918\n",
      "4.140994071960449\n",
      "5.5149827003479\n",
      "5.168785572052002\n",
      "6.52393913269043\n",
      "6.58537483215332\n",
      "6.492627143859863\n",
      "5.9318060874938965\n",
      "6.363910675048828\n",
      "5.730730056762695\n",
      "5.708500385284424\n",
      "4.305752754211426\n",
      "6.122987747192383\n",
      "5.579958438873291\n",
      "6.440825462341309\n",
      "5.252843379974365\n",
      "4.174548625946045\n",
      "4.611105918884277\n",
      "4.927542686462402\n",
      "5.019031524658203\n",
      "5.990700721740723\n",
      "9.5426664352417\n",
      "5.76188850402832\n",
      "6.393682956695557\n",
      "5.7164177894592285\n",
      "5.57454776763916\n",
      "4.140694618225098\n",
      "5.850538730621338\n",
      "3.765836238861084\n",
      "4.82191801071167\n",
      "6.106286525726318\n",
      "4.448174476623535\n",
      "5.034049034118652\n",
      "5.166525840759277\n",
      "4.810971736907959\n",
      "8.846052169799805\n",
      "5.592331409454346\n",
      "5.557619094848633\n",
      "6.312863826751709\n",
      "4.737827301025391\n",
      "5.3287763595581055\n",
      "6.460049629211426\n",
      "6.253903388977051\n",
      "6.0028181076049805\n",
      "5.347395896911621\n",
      "6.128130912780762\n",
      "5.6922221183776855\n",
      "6.277766704559326\n",
      "6.76394510269165\n",
      "6.131077766418457\n",
      "7.358763217926025\n",
      "6.24373722076416\n",
      "5.310696601867676\n",
      "5.540350437164307\n",
      "5.838226318359375\n",
      "5.22648811340332\n",
      "3.932063579559326\n",
      "6.998474597930908\n",
      "5.619207859039307\n",
      "7.017202854156494\n",
      "7.599421977996826\n",
      "6.78192138671875\n",
      "10.19040584564209\n",
      "9.193977355957031\n",
      "8.224176406860352\n",
      "5.0055718421936035\n",
      "8.010787010192871\n",
      "6.1299591064453125\n",
      "7.319950103759766\n",
      "6.8079447746276855\n",
      "4.941736698150635\n",
      "7.686788082122803\n",
      "8.093047142028809\n",
      "6.073788166046143\n",
      "9.179713249206543\n",
      "9.371479034423828\n",
      "6.445407867431641\n",
      "5.124188423156738\n",
      "5.093865871429443\n",
      "5.7065911293029785\n",
      "6.166507720947266\n",
      "6.227136135101318\n",
      "6.945775508880615\n",
      "8.689000129699707\n",
      "5.734858512878418\n",
      "8.587441444396973\n",
      "10.764161109924316\n",
      "6.4794769287109375\n",
      "6.793338298797607\n"
     ]
    }
   ],
   "source": [
    "# for tracking learning rate stats\n",
    "lri = []\n",
    "lossi = []\n",
    "\n",
    "for i in range(1000):\n",
    "    \n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, X.shape[0], (32,)) # creating a minibatch of size 32\n",
    "    \n",
    "    # calculating loss (forward pass)\n",
    "    emb = C[X[ix]] # (32, 3, 2)\n",
    "    h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # (32, 100)\n",
    "    logits = h @ W2 + b2 # (32, 27)\n",
    "    # counts = logits.exp()\n",
    "    # prob = counts / counts.sum(1, keepdim=True)\n",
    "    # loss = -prob[torch.arange(32), Y].log().mean()\n",
    "\n",
    "    # we can replace the above lines with a more condensed and efficient version\n",
    "    # it doesnt create new tensors like we do above but uses fused kernels for efficiency\n",
    "    # this also makes the backward pass more efficient bc the formulas can be simplified\n",
    "    loss = F.cross_entropy(logits, Y[ix])\n",
    "    print(loss.item())  \n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    # update parameters\n",
    "    lr = lrs[i]\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad # manual value is learning rate\n",
    "        \n",
    "    # track stats for learning rate\n",
    "    lri.append(lre[i])\n",
    "    lossi.append(loss.item())\n",
    "# print(loss.item())  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the collected data, we can plot a graph between learning rate and loss to see where the best learning rate is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x16c8b9010>]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAGdCAYAAACyzRGfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/OQEPoAAAACXBIWXMAAA9hAAAPYQGoP6dpAABrQElEQVR4nO3dd3hUZdoG8HtmMqmkEEISQu8g0gRpKoigwNqwrui66mdZXVzX3ta+urquu2vvfrDuWj6x7qprAanSBEE60gIECIGE9DblfH+EmXnPmVOnJpn7d11cTjnnzMmYzHnmeZ/3eW2SJEkgIiIiihF7vE+AiIiIEguDDyIiIoopBh9EREQUUww+iIiIKKYYfBAREVFMMfggIiKimGLwQURERDHF4IOIiIhiKineJ6Dk9Xpx8OBBZGZmwmazxft0iIiIyARJklBTU4OioiLY7fq5jVYXfBw8eBDdu3eP92kQERFRCPbv349u3brpbtPqgo/MzEwALSeflZUV57MhIiIiM6qrq9G9e3f/dVxPqws+fEMtWVlZDD6IiIjaGDMlEyw4JSIiophi8EFEREQxxeCDiIiIYorBBxEREcUUgw8iIiKKKQYfREREFFMMPoiIiCimGHwQERFRTDH4ICIiophi8EFEREQxxeCDiIiIYorBBxEREcVUwgYfR2qa8OriXThS0xTvUyEiIkooCRt83PDPNXjqv9tw/dtr4n0qRERECSVhg491+yoBAOv3V8b1PIiIiBJNwgYfREREFB8MPoiIiCimGHwQERFRTDH4ICIiophi8EFEREQxxeCDiIiIYorBBxEREcUUgw8iIiKKKQYfREREFFMMPoiIiCimGHwQERFRTDH4ICIiophi8EFEREQxxeCDiIiIYorBBxEREcUUgw8iIiKKKQYfREREFFMMPoiIiCimGHwQERFRTCVU8FHT6MKKXeXweqV4nwoREVHCSor3CcTSpa+txNZD1fjjzBPjfSpEREQJK6EyH1sPVQMAPv6xJM5nQkRElLgSKvjw4agLERFR/CRm8MHog4iIKG4SM/iQGHwQERHFS0IGH01ur/+2zRbHEyEiIkpACRl87Cyr9d9m7EFERBRbCRl8iGxMfRAREcUUgw+T2/1QXIGymsaongsREVEiSKgmY2rMJD5W7CrHrDdWAgCKnzo7ymdERETUvjHzYSL3sWznkRicCRERUWJI+ODDzLiLx2u8DREREZmT8MGHmZoPiX1BiIiIIobBh6nMB4MPIiKiSGHwYSL3wdiDiIgochh8mMh8sB07ERFR5DD4MLENgw8iIqLIYfBhIvXx0dqSGJwJERFRYmDwYfD85oNVqGv2xORciIiIEkHCBx9G0cfBSrZUJyIiiqSEDz58sYfXK6HJHZzhsHPdOSIioohi8HG85uPS11ZgxKPforbJLXveweiDiIgoohh8HI8t1uw9hgaXB8t3HvU/J0kSZr/zY5zOjIiIqH1K+OBDSZxUe6iq0bDYtLK+GS8v2omDlQ3RPTEiIqJ2IuGDD71BFTNt1e+c9xOe/mo7fvn6isidFBERUTvG4MNmky0cZ7Wf2JKfW4Zp9lcw80FERGQGgw8Av3tvnfCIxeiD9ahERESWMPiwAZ9vOBTv0yAiIkoYCR98KFMXVoddmPggIiKyJuGDD72lXcwEImZWxSUiIqKAhAk+JI1IQi92kKzWfxAREZGhhAk+tKbNltU0hXVcmyJ8OVDZAJfH67//6boD+O07a9HAxemIiIgAJFDwYaJlBwD5XBer9R/Ldx3FKU99h8vfWOl/7Nb/W48vN5ZizvI91g5GRETUTlkKPp588kmcfPLJyMzMRH5+PmbOnInt27fLtmlsbMTs2bPRqVMndOjQARdddBEOHz4c0ZMOhddkJCFJQFlNo+l9xJqP91bvBwD8UHwsaLvy2mZTr09ERNTeWQo+Fi9ejNmzZ2PlypX49ttv4XK5cNZZZ6Gurs6/zW233Yb//Oc/mDdvHhYvXoyDBw/iwgsvjPiJW2U2+Lj34w0Y88QCzN9y2HS2xEevfsTs6xMREbV3SVY2/uqrr2T3586di/z8fKxduxYTJ05EVVUV3nrrLbz77rs444wzAABz5szB4MGDsXLlSowbNy5yZ26RmVbpAFDT2LKq7Qvf7cAzlww33N7sZBfGHkRERC3CqvmoqqoCAOTm5gIA1q5dC5fLhalTp/q3GTRoEHr06IEVK9TXPmlqakJ1dbXsXzR4vcbbiDqkJulmPprcHkiSBJvJubbK2TYer4RGF4tQiYgo8YQcfHi9Xtx666045ZRTcOKJJwIASktLkZycjJycHNm2BQUFKC0tVT3Ok08+iezsbP+/7t27h3pK+udrMfXQISVJM1tSWd+MoQ9/g1//72rZ43pxiPJQ576wDMMe+Qa1TW5L50VERNTWhRx8zJ49G5s2bcL7778f1gncd999qKqq8v/bv39/WMfT4rEYfGSkJKkGLGv3HsPXm0vR7PFi6Y6jpoddlK+/5VA1mj1erCmusHReREREbV1IwcfNN9+Mzz//HAsXLkS3bt38jxcWFqK5uRmVlZWy7Q8fPozCwkLVY6WkpCArK0v2LxqsZj4yU5JU6zQuemW55j66Dcs0Xp+lIERElGgsBR+SJOHmm2/GJ598gu+++w69e/eWPT9q1Cg4nU4sWLDA/9j27duxb98+jB8/PjJnHKLQaj60uqLaxDumaMY+jD6IiCjBWJrtMnv2bLz77rv47LPPkJmZ6a/jyM7ORlpaGrKzs3Httdfi9ttvR25uLrKysvC73/0O48ePj+tMF8D6sIvTYTfX50O8rVP0oXUstnAnIqJEYyn4eOWVVwAAp59+uuzxOXPm4OqrrwYA/P3vf4fdbsdFF12EpqYmTJs2DS+//HJETjYcXotNO7xeyXKfD93jaRyLU3CJiCjRWAo+tOoWRKmpqXjppZfw0ksvhXxS0WC15sMjSdo/bwgr2Wq9fiQDHCIioraAa7to8HjNNSYTh1r0C07F25LqbSIiokSQMMGH2Q6nPl5Je9hFXudh/niB24HHxZdYu7cCM55bilW7y02fJxERUVuTMMFHt45peOuq0aa393h1hl0EsloSk03GvLLMR+Dxy15fia2HqvHL11dCSZIk1DezIRkREbV9CRN8pDodGN0r1/T2HpMFp9WN5gICMeCQZ2ECt10e7Re8+8MNOOGhr7H5YJWp1yMiImqtEib4AAC7hULRluAjco3BJI1sh9mSj3lrSwAAry7eHcKrExERtR4JFnyYjz48knbwcfeHG1Qft+mMu4hNzmTDLqbP6Pj2LFAlIqI2LqGCD4eF1IdXJ/MRCtmwi0YWxAyGHkRE1NYlVPBhIfHRMuxisSW72VVtJeG4ljucMvogIqI2zlKTsbbOyrDLvLUlqHd5Ivbakmyqrfq0WyIiokSQUJkPK8EHAHyx4VDEXlt72MVa9BHttWB2HanFzrKaqL4GEREltgTLfET3+B8en5GiRqvPRyTrSsLV7PZiyl8XAwC2PjYdacmOOJ8RERG1RwmV+dBbdTbatBqLuXV6e6iJZqzS0BwYZqppdEXvhYiIKKElVPABRD/7oUUMGsQmY60p8yGeSzwDNSIiat8SMPiIz0VVa6jFbbHi1EqssvlgFfaW15neXh58WDkrIiIi8xKq5gPwBR+xyTas3XvMf1tr2MVrNfgwee5l1Y04+/llAIDip842tY94KvEK0oiIqP1LuMxHrK6pOw7X4KJXlvvvaxWcWl1t12zmY/dR8xmPwLFbzxAQERG1XwkXfMTqG/3yXeXyBzRqPqwOu5gVShwha4TGQISIiKIk4YIPKy3Ww1FW0yi779FoLBatgtNQ+oF4wlhzhoiIyKyECz5iNexSVt0ku+/VaCxmueDU7HYhRA8ejxggBR/Aan0KERGRmoQLPmI17FJWoww+ArdlWZAo1XyEEjyI56WMco7WNmHMnxbgj59vMXcCREREGhIu+IhVLcPin4/I7osXfnHBOo/G4nUf/1gCl+qT5s5fWb+xcHsZhj7yNf7z00HNfcRaFOWrvLl0D47WNuGtZXtMvT4REZGWhAs+XBY7ikaKW6OxmEdj6dzbP/gJc74P/UIvKWpMrpnzA+qaPfjde+s099GaDgwAbq0oiYiIyKKECz6a43QRbXa3tC7fWVaDucuL/Y97dDIx3+8sD3rMbOJG1kvE5E5iq3dlwWq0ZuUQEVHiSbgmY1b7akSKL+iZ+rclssf1LupmJ+Z4vRIue30lOmY48dqVowHIgwezwYdXY0ZOy3ky80FERJGRcMFHvOyvaEBtkzvocbEWJDnJjmZ34CK/cPsRlByrR7eO6f7H1MKI3UfrsLq4wn88u90my3zM/b7Y1DnKaj4UAYvVBfDMkCQJN/5rLWyw4ZVfncT1ZIiIEkTCDbvE003/Whv0mNsrodHlwcHKBlng4XPGM4sNjytes13HMxRi5uLJ/27z305O0v5f7vZq13xEo1amvK4ZX28+jK82l6KynqvoEhElCmY+YmjpjqNBj3m9Eu75aAM+W68+C6XZ48V/Nx7y3/dlJFbsKkenDskYUJAJhxB9uDwSUpK0h1rSnA7N89MbntEqjA0Hm6gSESUmBh9x1uyRNAMPn5ve+dF/WwKwr7wes95YCQC4fGwPXD2hl/9536wUrSnFesGHR2NGDgC4WHBKREQRwuAjzrYeqra0vSQBxeWBRePeXbUPu8pq/fd9wyNaWYW0ZJ3Mh86wS7Sn2jK0ISJKHKz5iLODlQ2W93E65P/bVu2p8N92q9R8iFJ1Mh9unSZj0Sg4JSKixMTgA/pDEdHW0OyxvI9u0agv86GRS0jXyXzIFpZTznbhsAsREUUIgw8AHdOdcXvtuubg6bd6JABJOg1AfC3ZtWIF3YJTWc2H/Llo9PngzFoiosSUsMGHeBHu2jEtbudhNaEgSZLurBRfhkJrdorZYRflwEs0ptrK1rHj1BciooSRsMFHjpDteOS8IZg0oHMcz8YavS6tvsyHVn1ocpJ2ukGv4DQanWHlXVgjfngiImqlEjb4yE4LBB+dM1Pwj/8Zg3umD8KFI7vG8azM0au/8GUotDIfvqBCLdMgq/lQvmY0ZruImQ/OdyEiShgJG3yImQ/78eKDm07vi9vOHBC11+yaE/7wjiTJMxRKboPMh1eS8PRX2zDmTwtQVt0oe063z0cUhl28smGXiB+eiIhaqYQLPmaN6QEAuHFSX/9jdqHy0W52NbcQOCJwbAlSWJkPrwS8vGgXjtQ04ZXFu4T9vPh03YHA68R42IXBBxFR4ki44ONPF5yIbX+cjl6dMvyPiTGBI4pTMFwRGrrQCwR8s1KaVNaJAbQLO99YuhsLtx8RtlM/rp7D1Y2objS/Rov4Y5hdeZeIiNq+hAs+bDYbUp0OWYWBTZb5iN5rmxm6SHYYn4Bu8OGRsGzHUTz+xVbV57Wu8Qu2lsm3U9RgGMUGFXXNGPunBRj2yDf4YsMhU7NXxG0YfBARJY6ECz58xAtfa8p86DUQA1qCAP1hFy9ufu9Hzee1LvLKn1q5mVFosPlglf/27Hd/NFyvRvkajD2IiBJH4gYfwm2x5iMSdRlazAQfTof+6y/fVY5mneO4vVJQ+3WRVtyijLmCgg+L0cHyXcEr+Cox+CAiSkyJG3wIF7tYFZyaWR9FL3Dwmbdmv+ZzLo8XKTrZEzHzYQvKdwQEDbsYnJMyeDBTnyorOOVUWyKihJGwwYd4ORW/9eu1LrfidpUpu3oZCyuvv7e8XvM5t0fSHboRgwTx51YGIsHBhLXgwMzm8oJTS4cnIqI2LGGDD83MR4RqPkIdvrGZeP26Ju31YFoyHzrrt2hFBcphF8XTVluhm8lkiMdhe3UiosSRuMGHcFtWcBqhzEeoGRQzr1+rF3x4JaQ6zQ27iJSvqtxOMshSGNWMqJ+L/jGJiKh9StzgQyPzEanZLqEGMWb20+rhAQAbSypDKjhV0gsefIHJun3HcKiqQWN/My/EzAcRUSJKivcJxIukUfOhLDjN65CMo7XNhsfrmpOGA5WBC3FuRnLgmDbzF/1wEy8frCnR30Cs+RAeDo65lJkPeU+ObaXVuODl5QCA4qfODnoZUwWnsrVdiIgoUSRs5qNLdmCdFb06i+tO64Ntf5xueLwnLjhRdv/c4UX+23abzfQwTKRqTrTIZrvoFJxWNbgUa70EnpMkYN2+St3XMRNMsMMpEVFiStjgIzvNiQV3TMKyeybrbme3AanO4ALOOxSzWcQA5uoJvYKGPvQag8lfL7rBx5q9x1QfV77s/8xdgwtf/t5/X8wUmQkUzGzDtV2IiBJTwg67AEDfzh0Mt9EKBpQBSaRChmj2GVHj8ng1a0R+Kgl0LTUqOA1ipuBUKF1h5oOIKHEkbObDrLwOKaqPK4MEMUgJp3jSRI+xiPlyYykGPPBf/PsnE63QhdtqgYIyIGHmg4iItDD4MCDWboiUNRxigkR5HbUykhLNtWWUDlQ2QJKAW95bZ3iOsuJQb3Cw4FGsemsq+GB7dSKihMTgQ8eV43pqTn1VZj7Ee2YvpGrHjvWwi49eq3XAeAVaZfNW67NdGH0QESUKBh869C6IehkKsxfSVJU26NEuONVimPkQbnslKWh7jyLaMBOAyYtYjbcnIqL2gcGHDr0LqLI2Q9xUuZ9WVkFtFs3onh1Nnl0woxVxw6Gcdmu09ouZuhdOtSUiSkwMPnToXQ5dOivUmr2MqgUf3XLTMXVwvskjyCXZo/e/0ytECmqBRVDmw8Qx5Wu7hHxqRETUxjD4CJFei3OzF9IUjTVY+uYbTwFWkxRG5sNoQTuxT8m0Z5egye2RPR9u5oPt1YmIEkdC9/kwonc9VF58da+dGtf1VJXVZ20wLv7UoremixGtV3R5vFi8/QgaXIGf91i9C6t2V8i2U2Y+zNVwSCq3iIiovWPwoUPv23iTSzvzYfZSqrX6bKgTXkJdSRfQLji98Z9rsWBbWdDj2WlO2f1Qhl1kNR+sOCUiShgcdtGhl81oVswtDaVhllrNB2CtL4gonMyHFrXAAwCU5SWhDLtwYTkiosTE4EOH1pTZIUVZaHR5VJ8DIhB8hDzsEnrmw2rJhaKnWNDaNaam2hr0DiEiovaJwYcOtethdpoT/7p2LM4YpD0jRRm0aIUEWsFCyMMuYWQ+Fv98xNL2ymBDOWxibvE54U6cYg8O9xARxR6DDx1ql6XzhhehY0YyTu2Xhw9+Mz6wrdTyHABce2of2T5ajcM0g4UQx13CqfmwSjmsoqz5cAtTkTcdqMI/VxQHXejj3WRsTXEFhj/2DT74YX/sX5yIKIEx+NCh9+XdZrPhpB45sseeu2wENj86DQMLMxXbqh9DK1gQH716Qi/jEz1Or+ZjcJcs08cxQ5nZULY9cQnjMue8sAwPfrY5aAE7+Uq5sY8+Zr/7I2oa3bj7ow0xf20iokTG4ENHUU6q7vPKjIbNZkNGSvAEIq18hFZTMPG4ZwzKx13TBuqfqO94OjUfkc6JuBVNx5RZDbdKE7atpdWy+/EuOA21toaIiMLD4EPFnGtOxqWju+Gm0/vqbqe3kq1Ic9hFK/MhPKy1sJ0ap06H00gvGaNckdajyFxsPFCF/RX18nNQXOy9cS44jdMafkRECc9y8LFkyRKce+65KCoqgs1mw6effip7/uqrr4bNZpP9mz59eqTONyYmD8zH0xcPR3qyShbDJt42efWyAXkdkoMeTlZZWA6QXxStLDSnl/mI9IJ18rVepKCaD6BluEWPpHknNkz//yMiooiyHHzU1dVh+PDheOmllzS3mT59Og4dOuT/995774V1km2BXl8LG4D3bxgX9HhBVor69sJF0WG3aWYtbpnSH/NuDBS96s12ifR11iPJi0XVZo1UNbhU991ysBqvLt6FZqFFfTwyH1aySkREFDmWO5zOmDEDM2bM0N0mJSUFhYWFIZ9UW6Q77GK3oV9+ZtDjRTlphsfVmz07qDATHdMDnUb12nxE+lu+bFE4SEHDLj5V9YEAxHcKv3h+KQBgTK9c4XgRPT1TGHsQEcVHVGo+Fi1ahPz8fAwcOBA33XQTysvLo/EycRHK9UpryEMr+BC31xsuSbLb4BDqPPS2jfR1Vhxm+WHPMVQ3uFW3G/7YN/7bP5fWwC10hv1x3zH/7fjUfDD6ICKKh4iv7TJ9+nRceOGF6N27N3bt2oX7778fM2bMwIoVK+BwBHf0bGpqQlNTk/9+dXV10DZtndYlrqtG8KFXcHru8CL85/iU1SSHTVa0GssaBnEyy6/eWmVqnwXbynDHvJ/892UzZiJ2ZubZmfogIoqLiAcfl112mf/20KFDMWzYMPTt2xeLFi3ClClTgrZ/8skn8eijj0b6NGJPtyeI+uMFWepTeZUFp+Iske4dAwFLkt0uC070Yo9IX2fNrN2i5rP1B1UfD/V44WDsQUQUH1GfatunTx/k5eVh586dqs/fd999qKqq8v/bv791d5sMJbugtY/WbBcx2FAODaQJ68Ek2eWZD72LaaSzImqzW8IRjw6nHHYhIoqPqAcfJSUlKC8vR5cuXVSfT0lJQVZWluxfe6N2ifvH/4zR3l4x7CLeT0sWgg+H3fSMjcFdggtezZo6OHgdm+W7IlvHo0x8rNt3DCXH6tU3jhAGH0RE8WE5+KitrcX69euxfv16AMCePXuwfv167Nu3D7W1tbjrrruwcuVKFBcXY8GCBTj//PPRr18/TJs2LdLn3qporYALBA+HZKYmYdKAzgCAU/vlqWwvTrWVP5ciZj4cNlmXVL3sgXK9GSs6Z6Zi4yNnhby/GWLB6c6yWlzw8nKc+ueFUX1NnZ5sREQURZY/ftesWYORI0di5MiRAIDbb78dI0eOxEMPPQSHw4ENGzbgvPPOw4ABA3Dttddi1KhRWLp0KVJS1HtatBd5HbR/PuU3bDFb8eZVo/Hf35/m7/kxaUBnWaakpeYjQDns4hDm12rVTfTtnIFUZ+hXWocdyEx1Gm8YBvHMN5RURvW1fJj5ICKKD8sFp6effrpuceDXX38d1gm1Na9dOQr7K+oxrFuO5jbKS5xDuOilOh0Y3CULi++ajOpGF/IzU2W1G8phFTGISLLbZTUfWpkPm80W1oXWEYOLtKxvSIzqPxh8EBHFBxPPYZo2pBDXnaY/pKEs9lSb4pnqdCA/MzVoe+UFMjVJPuwiBifKItA//GIwnA4b/nzRsLD6fFiZknrjJP31cLTEY5E5znYhIooPBh8xoPyCbZRJ0Ovzkaoz20XZqOv6iX2w9bHpGNWzY1hdxnzne1r/4PoUpc6ZoQ2vaS0yV3KsHre8tw4bS6pCOq4eZj6IiOKDwYdFoVyvlPsYfePWW9slRTHsIm6r1iVUb70Xs3x1Jf+4Zgwm9O2ku61Yk2KFfJXcwJ2b312Hf/90EOe+qL9IXSjYZIyIKD4YfMSAcin5PIPsgLLgVKQcdhGJC7UphVNH4ct82O02dEwPXp1XpNW7xIhXo+ZjV1ltSMczg7EHEVF8MPiIAd9F7q2rRmNkjxw8+8sRpvd12OUdTmUFp7EKPsRGZgZX7FCDD/H8YtVwTGvY5UhNE/6xvFhzVV4iIgpPxNurt3fKLIapfY5f5KYMLsCUwQWG24vXXuW13umQD7uImnSCj3B6WogXab2VcwEgJdTgQ/ip5bejR6tB21X/uxpbDlVjxa5yvHrlqCieARFRYmLmw6KTeuZY3sdyuCKkAZSZBjHbobx4Nnu8sgJUUX5mKmaN6WH1TIJex6hIM/RhF/XbkW7jLtJqOb/lUMviht9uPRy11yYiSmTMfJi06M7TsfFAFc4eqt4mXo/VIlXxequcGSNmPpTHbXZ7keSwyVaLFT154VBkpznx6uJdls5HDD6MQoGQMx8aBaeeKDb9MKr5MNu6noiIrGHwYVKvvAz0yssIaV+ri7qJF1/lbJf8zBRMHVzQ0nU0Rf6/r8nthdNuRyN0hl9CuJ6K2Q6XR/vYQOjBh6/g9IfiCjz02ebA41HMfBhNedbKIhERUXgYfMSA1UuYeL1VDnPYbDa8edVo1f18mQ/dcwlzqrDRMEiyw4F3rxuLy99cZek1fEe95NUVssejmfkwCgqZ+SAiig7WfMSA1WZWXkXmw6yW4EP/f2m4jbW0hnR8Upz2kOo+tFr2R7PVOoddiIjig8FHDIRzvbdy/WtyewyHCsK9nLoNhl2SHfaQmnfFaj0XkayWRa1BG4MPIqKoYPDRComZD5vNZrpmxCsF9/5QuiyEGS/iEc1kPkLJrqh1Z402u6w7bPDzzHwQEUUHg48YsHoxDuc67DRo6FGUk4YzBuVbOqZ4+m6PUc2HPaRVcGPVWEwk+7m8wRkdZR8VIiKKDH66xkA4U22tMsp8AC0BQqjULtKyYyfZQxpm0qr5iCYxs6H2YzHzQUQUHQw+YsB68GHtQnz52B7+/5r5ti6F0TdUa9glPdmBO84cgMxUZ0jDLlZ+ZEmScPWc1bjuH2vCClrE81TPfDD4ICKKBk61jYFwZ5gY7f3IuUMwc0RXjOieg0teXR7Wa6m/fuAMtKbazp7cD7Mn9wMQWsZAgoTSqkZT25bVNGHR9iMAgOpGN7LTnJZfDzCeQszMBxFRdDD4iAGrlzCr3+aTk+wY0zsXAAyn2rYc3+IJCVwaNR9igBXKNdsrAXd9+JPJbSMzRCOeM4MPIqLY4bBLDFjvcKrc3/y+d541EADwq3GhreOiRp4hUK/5EGOeUKfa7jlaZ+58wp4wHHhNH7Xgw0z9DBERWcfMRwzEsuB0fN9O2PDIWUGt10Xh5A20aj7kmY/YTbW10n690eVBqtPhvy/Wvqj9XA7OdiEiigp+usaA9fbq4Q0rZKU6dbMt4Rxea6qtGHCEMtUWMB+kiUHDez/sw+aDVYb7LN95FIMe/Ap/+2a7eCA/1cwHh12IiKKCwUcMWO7zEaXzUHuFWRabjmkVnIrX6VBiDysZDPEcnv5qO85+fpnhPg/9u2Wxuue/2xl4TXH1XNZ8EBHFDIOPGLB6MY5lzwuniboGm8GUVEB+oQ6l5sMjSaZrOYwWt1Oj9p6Kj6gtYMfMBxFRdDD4iAGrBZLKYZdIXwJP6ZcHoOXiaqYviPj6WpkSW5jDLkadU0WhBR/6j/mOKYW4qB8REZnHgtMYsJ75iM55+Fw5ric6pifj5N65eHt5saV9b57cDyf3ysUVb66SPS7LfCiGYMz8PC6P1/T7FEpNjNoe4nF8wY9YeMrgg4goOhh8xEC4s12sTtU1kuSwY+bIrgAAp8VW60kOuz9zIhKv0+KwS1aqE1UNLsPjNrn127aLDBbWVWU07OILRFzCwTnsQkQUHRx2iYFBhVmWtg+n/blVZnpZmIl9bBpTbTNTzcW3LgsRhdqwy8YS/Rkvqu+o8ODin4/g3o824Fh9IFBi5oOIKDqY+Yiiz2afgi82HsItU/pb2zGMJmNWKTMfqU47Gl1e9MnLwO7jTb/MvLxDo+ZjUGEmSo41GO5vJvioa3IjzelQDT7OfXEZip86W3NftZEacdjlL1+3TMEVMzAMPoiIooOZjyga3j0H9/9iMDroNPxSc9WEXkh22HHp6G4AgHOGFSE5yY4pg/Ijfo7KoYVPZ5+C80cUYc41J6NjesuaKRMHdDY8jli3ahNu/2pcTyQnGf+aNbu9hkHOkIe/xqw3VqrOTDGilk1SO8xuoctqHBbaJSJKCMx8tEJFOWnY9Og0/0U7NyMZmx6ZZmparFXKtWAGFWbhuctGAgC+v/cMlNc2o3tuuuFxtDqcJjvsePCcE/Dgp5t099daM0Zp1Z6KkGa7qM0QVgtI3EIGJlJryBARkRyDj1ZKmS0wkz0IhV5Ak56chPRcc78imh1ObeYWmmtye00X1oY020WxT2V9s+r0XjGwCafNPRERaWPwkeDM9PkwQ6yPkMceNlMdXq0UnDa5rE93EeOI3UdqccZfF6tuJwYfsWz2RkSUSFjzkeBG9+oYkeOI2Q1lzw8zTcfM1Hz4mJm6qyTGEZ+uO6C5HTMfRETRx+AjwQ0oyMSI7jlhH0er5sNms+FIbZPh/lYyH8/O/9nayUFe36E3vOPysuaDiCjaGHwQhnfLDvsY8j4fkN0e2SPHcP9mC8HHjrJaK6cGQJ750EvEeDzMfBARRRuDD0KK0xH2MTxCxsCmyHyM79MJH/xmPP526XDN/ZstdDi1SpIklNUEsi96NShulZoPr1fC5oNVIc2yWbfvGH77zlqUHKu3vC8RUXvF4INw46S+GFiQiftmDAr5GFrt0W22lgBkTO9cZKY6Nfdv9ngjv4LecQu2lsnu682+kdd8tNz+yzfbcfbzy/DofzZbfu0LXl6OLzeW4uZ311nel4iovWLwQcjNSMbXt03Ebyb1Nb1PQVaK7L7WDJQ+eRn+23prpVip+dDy1rI9+GDN/qDHv9x0SHb/nVX7NI8hz3y0/PeVRbsAAG+v2BvyuRWX1xlvRESUIBh8UEg+m32q7H6T2yO7/8MfpmLp3ZORk57sf8yuE3xYme2i5Y+fb8HdH24ImiJrUxz5UFWj5jHUMh+ihz7bhOpGc7NtyoVCWy8LSIiI/Bh8UEgKs1Px2pWj/PeVwy6dM1OCOqPqZT4OVzehMYT+HWqU13krS7S4ZbNdgp9/e8VePHN8HRgjox6f77/N0IOIKIDBB4Vs2pBC/+0ME+vXGDUbO1BpvACdGcrCUDNNztT21WoytudoCEMojD6IiPwYfFBY/nLxMJw7vAgXntTVcNskjVbuE/p2iug5KYdL9IZ7lNwmmozpZXDMnhMRUSJj8EFhuWR0d7wwayRSkoyn64oZiL6dA4WoV4ztGdFzUmY+LCQ+ZP1AtAIGh4mW9MqsCUMPIqIABh8UF09fPBx9O2fgb5cOR75i5ky4PMKF/6f9ldgb4kyTcDIfygCImQ8iogAuLEcxI2Yg+hd0wII7TgcArN17LKKv45tZcri6Eee/9H3Ix9Gq+XDorATsPwfFrow9iIgCmPmguBCzB44Qaij0+Oo2QioMFWhlK7QyH5IkBbqiKoddGHwQEfkx80FxIdZ/mFn11oqGZo/xRiZ4ver9OdTOV5IkXPb6SgDA+zeMCw4+WPVBROTHzAfFhZjtMKrfnDmiyNKxT3t6ITaWVIXdtEwC8M+VwV1NxXNfvvMoHvh0Iw5UNmDVngqs2lOBirpmlZqPME+GiKgdYeaD4kLMHiiHXa45pRc8XsnfzjzVYOG7oV2zsfFAleyxv8//GTdM7BPWOUqShH+pBB/ilOHL31wFAEh2BM7RbrOp1Hww+iAi8mHmg2JGDDHE3hvKYYzfnt4P/3NKb/99o+BjcJfMoMckScKSn4+EdqLHeSUJTkfwn4gvWDooNEXbL6xaKyE42GDmg4gogJkPipnMVPVfN2UTsOQkO7xSIOAwCj56KNq4A8Deinos3B5u8NFyLkpJx8eJfj5c43+sVFgvxu31RryOhYioPWHwQTHTLz8Tt07tj7wO8r4eygt1iuKCb9RXQ63B2e4j4a8i65UkJOtkPvYfC2Q+SoTMh8crwRbhGTxERO0Jgw+KqVunDgh6TFnz4XTYLU2/TXFGZ/RQkgBnUvB5+IIhMeBoFhbWc3sk2G0cZyEi0sKaD4o7u6Lnh8Nuk9VaqE1THVKU5b+tVpcRCcY1H4GhFnFVX49XikhH02a3F49/vgVLd4Q3fERE1Now+KC4E4ddnBrdQ6+e0Et2f+rggsD+URri0Ao+fJmPirom/2PignRurxQ01TYU767aizeX7cGVb60O+1hERK0Jh10o7sQ+H2o1FgDw8Lkn4EhtE77YcAgA0CEl8KsbyiqzZuyvaMD+iobgJ44HS8fqXKr7ebxSRDqalhxTeW0ionaAmQ+KO3nmQ/1X0mazISvV6b+fIQQf0cp8aPFNo62sb1Z93u31RmTYRTkLaMfhGtn0XiKitoqZD4q7JCH1obzgAsCI7h2P3wpc0DNSAjNckkwscR9JvsCiQiP48ERo2EV8J47WNuHMvy8BABQ/dXbYxyYiiicGHxR3YuwgDqHMv30Sth6qxtTB+QBa1lrxGd0r13871pkPrwQ0ujxodHlVn3d7pYg0FbMJGaG95UITM0mSPUdEZIYkSXj66+3o1Skdvzy5R1zPhcEHxZ1snRfhotovvwP65Xfw3xdnvXTNScOCOyYhO82J9fsqY3KePl5Jwl0fbtB8PlKzXcSYSizEdXslzcJcIiItP+6rxCuLdgEAgw8iMeBI0rmoOhTDK307twQmjhhfiN0eCf/56aDu85EJPtRrYZrd3qhNLyai9kurTi0e+AlGcefQWedFdOvU/ujWMQ13Tx8oezxas120uD2B4ZbJAzsHPR+xmg/hxxJ/RrGnCBGRWa1pjSlmPijuxIBDreDUpyArFcvuOSN4/xgHH82ewF/wDRP7Bq0h4/Z6IzLVVquuo8ntCf/gRERxxMwHxZ0YcISSxYj1bBcx85Gq0trdK0W+5sMjHK9ZJ/MhSRLKaho1nyeixKVcbTueGHxQq2IPYRZHrDMfLiH4UKu9cHtCG3bxeFtqSXy9PMT3Qpzpozfs8si/N2PMEwvw6boDll+fiNq31hN6MPigVkav4FRznxgFH74gxyUEFmrBh0djqm2jS3+45KMfS/C799Zh4tMLAcj7fIiZlCaNKb4A8I8VewEAf/5qm+5rEVHiYeaDSENrzXzcfuYA3HJGfwCBYQ+bTf213RpTbc9/8Xvd11hbfMy/PyAfjpIFHyZqPlrRZwwRtRKt6XPBcvCxZMkSnHvuuSgqKoLNZsOnn34qe16SJDz00EPo0qUL0tLSMHXqVOzYsSNS50vtXEg1HzGYamu3BWowfMMuScdX4FXyeCV4VVIf2w/X6L5G58wUxWuKwUfgcb2aDx+1lYCJqH362zfbcfeHPxlmNlrTp4Ll4KOurg7Dhw/HSy+9pPr8008/jeeffx6vvvoqVq1ahYyMDEybNg2NjSyCI2N6s120xGLYxW63+c9t0fHZLQ67DWov7fZKsgJRs/I6JPtvN7o8sqm2Yg2Jmam2rekbDhFF1/Pf7cQHa0qw+WC17nbi50K8h2AsT7WdMWMGZsyYofqcJEl49tln8cADD+D8888HALz99tsoKCjAp59+issuuyy8s6V2L5RAQtl8LBrsNhu8iu8NDptNdZjIE+JU2w7Cwnll1U2ywEYscjU17GL95Ymojattcus+Lw7fSpK8l1CsRfRTe8+ePSgtLcXUqVP9j2VnZ2Ps2LFYsWKF6j5NTU2orq6W/aPEFUr9RriZj4fOOcHwj7Bl2EW+kUPIhojcJpqMldU0YuH2Mtm3D/F2VYNL9npuj7XMBxElBvFzQ/ycUN1WuB2JdgDhiGjwUVpaCgAoKCiQPV5QUOB/TunJJ59Edna2/1/37t0jeUrUxoQSfISyz/g+nfy3Tx/Y2V9MqsVuCx5icdhtqh1ZzaztMuWvi3HNnB/wiTAlVtzF5fXKmozJMx8cdiGiFsrPDf1tAxvHu9tp3Ge73HfffaiqqvL/279/f7xPieJIr7265j4awYdYQ6H0h7MHY8ldk/HhjePRp3MHw2EKu8oQi8NuV6/58EiGF/+axpb06IJtZf7HxIDF45VkU22bLQYfRJQYvBYyH6J4F6VHNPgoLCwEABw+fFj2+OHDh/3PKaWkpCArK0v2jxJXKAWnWsHH//1mPG45o5/qczYb0KNTOkb3ym15wCBaUB92UW+BbmVtF3FWjLjL/op6vL2i2H9fzHyYme3Cqg+ixCB+boifE+rbyms+4imiwUfv3r1RWFiIBQsW+B+rrq7GqlWrMH78+Ei+FLVTobVXV9+nR246ZmsEH8pAwujvUG1mS5Ldrhr4PPHlVrywcKfBEVt4ZWnQwO3bP/gJxeX1/vvPzg9MV+faLkTkI35uGAUfYsAR75oPy7NdamtrsXNn4IN1z549WL9+PXJzc9GjRw/ceuutePzxx9G/f3/07t0bDz74IIqKijBz5sxInje1U5HMfDgddtV+G4D1ZmY2mw3KSTV2O1SHXQDgp/2VQY9lpgb/uYmfFXofBTvLav239Tqc+o/FxAdRQpDVfBgVnMqCjyidkEmWg481a9Zg8uTJ/vu33347AOCqq67C3Llzcffdd6Ourg433HADKisrceqpp+Krr75Campq5M6a2q1ILyynFcwoHza6WNtttqDgIMlutxQsJau0YvdqzHbRY6bmo7yuGct3HcWEvnmmz4+I2h6PrObDIPMh3m5rs11OP/10SJIU9G/u3LkAWr4hPvbYYygtLUVjYyPmz5+PAQMGRPq8qZ2KZMGpHq3l6rVfIzhgUasD0aPWeMzl8eLHfcfg8ng1szRK5mo+gMvfWGX63IgotvYcrcP1b6/BepUsqRWyYReDzxAvZ7sQqYtVnw/l64iV33OvOTloexvUZruoT7XVolaEunTHUVz48nLc//FG0x8GrPkgavt+8881+HbLYcx8SX/NJyOS8F3EZfTFpBV1OGXwQa1KKMFHKHUiesMupw/MN7WP12KHQL3Mxry1JaYLwDjVlqjt2ysUlIdDNtVW0eej0eWRFaGKX7KY+SASnNSzY0j7vTBrJFKSzP86W53tIkEKGqrxeCVLwZJHktDk9mB/RXgfOmaHXYio/ZPPdgncrm1y4+Qn5uPiVwPdxdv02i5E0TD/9olYU3wMF5/ULaT9zx1ehA0llXhj6R4AwDvXjdXd3mppiSQFD++4vV5LNR+NLi8GPvCV5vPmMx+hD7vsLa/DC9/txI2T+qBffmbIxyGi1kGsJROzost2HEFNo1s2887Timo+GHxQq9AvPzPsi6E4/HKyr3mY1rbKzIeJP0TlpBqPR9KcahsK8zUfoWc+fvPPtdhWWoMP15bg6gm98Mh5Q0I+FlGi8HgllFY3omtOWsSOGalrv/jZJWZFi1WGdcSh33hnPjjsQu2GWPxpNBxitc+HpLKP2xs8FBOO9fsqTW0XzrDLttIa/+25y4tDPg5RIrnlvXU45anv8NWmQ/E+lSBeWeYjkBUVa0p8gYZHo6NyPDD4oHZDDDiMMhLKLIaZdQ6UgYY7wn+9X21WX3xRKZzMR5rTEfK+RInqi40tQccri3bF+UyCeTUyH/XN7qBtxB5k7WptF6J4EjMTRhmJoMyHib9D5REbXfGZ8hpOzUdaMoMPolA1W1i4LVbEoRTxi4n4eeXLjmitJRUPDD6o3bAy88TysIsUPI2tIU7BRzjDLsx8EIWuuRX22BFLN7Syor7hFlnBaZyjDwYf1G5YCz7k981MtXW55VvFq16r0cTaLlpSnfyTJwqV0dop8SAGFFrBkVel5iPe+ElE7YaV4COUQlGXt3X01/B4JTQ0e1BZ32x5X7Vhl5W7yzH92SVYU1wRidMjareMVo2NhCa3BzOeW4p7P9pganuvxlRb+TbH/ysbdmHmgygitFqdL7lrMv580VDZY8EdTo1Xg3S3km89bq8Xox//FiMe+xZVDS7/4x//WGK4b2pScPBx2esrsa20Br98fWVEz5OovYlF8LFo+xFsPVSN93/Yb2p7SZb5sDDswpoPosjQarPeo1M6Zo7sKt/WoM/H9CGFQceJxQePGR6vhLrmlvTq5oNVAFo+gG7/4Kegba+ZsxpXvLnS/wGVqlPz0ZpSskStUSyWNrDaf8NrouZDkiRU1jfLZuvEO/PBJmPUbjh0RlJsirkqRpNdXrriJJQcq8ekvyzyP9bcSoIPtSm+Wue2cPsRAEBZTRMKslKDfu54F50RtSUR/QISoT89r8nMx53zfpIFJ3GOPZj5oPZDr+bDaidSh92Gnp0y/PclIKjgNF5kK1cePyWjQjhf0BG0Po3BJ5DL48X/LtuDnw/X6G5HlAgiWXAaqT4bYimaOA1fPLpXCnwR8T/PzAdRZDiUncMEyotuskO+reHfoSTJvvVkpznx4DknWD7HSBC/vfgSF6FOvzVKvc79vhhPfLkVAFD81NkhvQZRe9Eahya1Ck7FU1X7O4/3j8LMB7UbDp3fZjHz8dSFQ5Gkt7GGc4cXAQAGFWZi/UNn4uJRwYvgXXNKL8vHtUp12MUg+PB9aCqHWYwm8KwXFqUioliwlqbVGnbxyopL1YIPznYhioiO6cmaz4mZD3E4xccoBSoBGFiYidX3T8G/bz5Vc6ruQzHOhri8Xvzx8y2Ga058tLZlJoxyzNpo2CXeLZiJSJ9Wwan4RUMtYxPvmg8Ou1C7MXVwAS47uTuGdcvR3U4tbjD7h5iflWpw7Aguc2vCvDX78eVG4zVhnvnmZ9w4qW/Qh5DRt594f0ARJRqrHyFmMh9qf8fxznww+KB2w2634amLhkX0mFMHF2DJjiM4Z1hRRI8bKXtVls3WUnKsAa6gYRdGF0StidWvL5Ks5iNQcComOZn5IGpj3vj1KLg8EpKTWucIpZVC091Ha+FRFHkYxR7x/oAiSmSSJBlmU8Ugw+WR4PVKsNttsqBELcsR7yHV1vmJShRFan/KWtPObDab5cDjratGh3BWAVdP6GV6Jo2VpkcHKhuDurQaVe/H+wOKKJGZSUwqAwuPJMHjlfCDsFwCZ7sQJYApgwvC2j8n3Ykrx/U0ta2VzIfL7Q2aKcOaD6LWy8zUXuXfsFeS8PLCnahudAuPGe8Xaxx2oYSjlsaM1Z/hrDE9UN3owhcbtGen2GAzvUiela6rbq8Xbo9y2MV4lg8RRZ7Wn574+WQmQFBu8rt312Hh9jLZY+o1Hxx2IYqJc4Z1wdCu2TipR07Qc7H6O7TbgCQhsHjqwqGq25jtyNrkUl9CW43LIwVlPgyHXRh9EMWNmb8/ZYDyzZbDQZ1YW2PBKYMPShgvXn4S/n3zKaoNxqae0DJUkpPutHxcK1PjHHabbPXd7rnpQdvY7TbZt5+zh3XRPF69heDD7ZGCPoT+uWKv6f2JKPrEjxOjPjyAudoN9am25s8pGjjsQglFq3J8Yv88fPLbCeidF9yAzIjTbjc9/GG32WSr7zpVAiHlKXbUCYisfHtxe71B34heW7Jb83WPv4L5FyCisIl/h2aGXcxMl1cLYljzQdQK2Gw2jOzRMaR9kxw2NJtMQNgUwy5JKkvx2hVRQKQ+I1weKWiqrezchNs/H67BW0v3YH9FQ2RenIgsk0x8pzEVoLTC4IPDLkRhSrKwZK5DkflQ21f5iFcCnv3lCEuvo8bt8QZNtZW9rhD0nP/i9/i/NfuxPUqr2Ta6PLjw5e/xt29/jsrxidoDc4GFieOobcSaD6K2TW3oZM0DU1W3tStqPpRZDiD4w0SSJMwc2RXb/jgduRna69cYcXuDC05F4pk0WKglCcXHPx7Aj/sq8fyCHVF9HaJICDPut0SMN8zUfHy1yXh5BfWptlbOKvIYfBCFSW3oJK9Diuq2Nhtk02jVptQqG3v5vv0kOeyoqGsO+TznLi/WDSpCWZZmQ0kldpbVWt5PbANN1NrFcs0m8a/fTObjox9LDLdRm+3CYReiNi7Jbv7P6NR+ecbBh+Izwcw3lF6d0nH2UO1ZMWa4PBI2HagyPf+/rKYR5734Pab+bTEAYNeRWlz/9hpsKKn0b1NR14w3l+7G0dom2b7xnuZHZEUsl4s0WhAuFGp/0ww+iNo4p0rmQ2nV/VPw7vVjcVr/zrKAw0w618yHxIuXn4SMFIfxwQyc88IyfL35sKlt9ykWtbv+7TX4dsthnPfi9/7HbvrXWjz+xVbc/O6PYZ8bUbxEI/Gh9VctG3Yx7MNjLoBQG76Jd/zP4IMoTGp9QwDg3evHolvHNPzjf8agICsVE/rmAZDXeajWfCg+cMx8vqQnOzTPwyozaVwAQdN21VbYXbWnZX2JlbsrZI/H+4OPyApbDHMfegvC7Tlah6l/W4yP1rb8jTa6zE3xV603ZeaDqG3TmoUyoW8elt1zBiYN6Ky5vXrNh+K+iQ+JjJQkOCNUFWembwAAvLVsj/+2JEmWivLi/cFHZEkMx13EPz/ln8kDn27EzrJa3DHvJwBAbZMbZqj9TevMuo8JBh9EYRpSlG1pe7vdaLaLsuDU+JhpEcx86M2I8fF4JczfGhie8UrqP4tPcoTOjSgeYjnbxauT+ahrkhdq1zebDD5a4bALm4wRhemhc05AZmoSLjypq6ntxam25gpOjT8m0p0O1Sm/odB6vVcX70J6sgO/Ht8LbsXXJo9X0g0+0iNQj0IUL+Kwy5tLd+O60/pE7bXEvz9lzYfyT8xs5oOzXYjaoex0Jx45bwiGdcsxtb04NVftgh087GLmmHakJJn7czb6Fqf1ofTUf7fhoc82o8ntCfow8xoMu6Q7GXxQ2yX+mT7+xdaovpb456eMGZSfF8pMiBbVzAeDD6LEIis4VfkLVH4omP2Gkmwy+DAantHrggoAjc1e9eBDJ/pITZYHHyz5oLYkflNtFZkPxbbLdh41eUxzj8USgw+iGBOv/WpV9HrDLh/8ZjzOH1GEqYPzg/YTMx95HbQ7oRq1aTcKdupdbpXgQ7/mIyOZI7zUdkWjyZhW5sFsh9P1+813CFYbdon3FwB+IhDFmMOgKVlwh9PA7TG9czGmdy4AYPa7P+KLDYfwq3E9AAApwtDGN7dNQoPLg59La3DN3B9kxzMKPn4oPqb7fG2jO6i+xOOVVOtXfNKUmY+4l7sRmRevzIdyRooY4K/aXR7SMfUeiyVmPohiLFmo+VC7CAev7aJ+nD/NHIrnLhuBB84+AYA885HmdKBrThomD8rHH2eeKNsv3FkxZ/59CbYeqlacY3DNh8sT+OTskJKk2D6sUyCKrTg1GQsKEITz0Av2ldQCjZJj8V2xmsEHUYxdeFI39MnLwFXjewZdlIHgC7NWejY73YnzR3RF6vGMhxh8iFlij0f+9Snc1XEB4IFPN8nuq812OSasQ5OVyiQrtV3K3+1oFmtqtVdvdHmwek+gWZ/eMKdSfXNwYeqfv9oW2glGCD8RiGIsIyUJC+6Y5B9HnnfjeNgAXPzqCgDBH2wndjXXR0QMPsQPJmXfjkgEH8pupmo1H0drA8FH0Awe8bYkxXThLiKrlL+eTW6vP+iPNK9GzYfY1A+wlvmoMzklN5aY+SCKA/Fie3KvXIzuleu/7/u4+erW03DP9EG46fS+po4p1nyIn0sTlR1Wo9Dwa/7Wwyivky8eJ67A65XkQZXedEKi1kaZ6GhQySREilaTsf0V8oBfnF1mtKik2Sm5scTgg6iV8V2kBxVm4abT+5r+hqWV+RhQkIlXrjjJfz8SmQ+l+z7eKFvrZeLTC7G6OJAiPlzViDF/WoAnvtgStG+8C9+IjCizkXUmO4v+c0UxznhmEXYdqTX/WhqvGzRjRXjO6DNCLfNhZkHMaGLwQdTKhHotTkkKfAAp08TDu+f4b1tJ14ZqX0W9bBrg6uIKHKlpwhtL9+CfK4pRVtPof47BB7V2RpmPb7ccxn83HpI9VtfkxoOfbcbuo3V4weSU2JbXEgOOwH1l7CEOpxoFEmrBUqeMFNPnFA0MPohamVCHIcSiTmUNRVaa03/baJnuaHvws82Y832x/77X25JSjnfHRSLR3vI6LNxeBiC4ZqnBFQg+mtweXP/2Gtz0zo+orA8MNR6sDMwm2VZaAwDYWFKFuz/8CWXVgeBbSVwEbs3eCox+fD4+XFsSNDNObAZoVDNVqzLsMqybtTWpIo0Fp0StTKiZgH75HXDluJ7omBHcYCxD6LOhVvkeDdlpTlQ1uAy3e23JLjw7fwd+P6U/bjtzQAzOjMjYpL8sAgC8d/24oL9JcYhRvF3T6EZOesvfX7Mwy2xbaQ12H6nFuS8uAwAcqZHXR63cXY5xfToBkH/5ePqr7QCAO+f9hAtGyteOMrMApE9to/zvcGBBJv504VDT+0cDMx9E7YTNZsMfZ56I21Uu4OI3o/pmt2GBWiSYzWQ8O78lJf3cgh14dv7PWG6yZTRRLKzYdTRo2MUtBBZav+fKZQqm/G2x//aOMnkNyDVzAo0Atb58KDOWHmUHMh3VjfJhl5d/dRLyOnDYhYgE0R5+qGv24IVZI7Hq/inISXca7xCiJrf5D0efZ+fvwOVvrvLf93olfLBmP3YcronkqRGZVtPk1s18aMUALkV/HfEQymnp4jCO1p+/MmPpkgU3+p8ZNYrMh6MVTG1n8EHUykS78sHjbVkEriArFS9ffpLpBemsCiX4UPp84yHc/eEGnPn3JRE4IyLgcHUjfvHcUryzaq+p7eua3EF/k2Jg4dGYQt7s0f79t9u0gwytzEfJMflUWyt/XzsVmZZYFJ0bYfBB1EqcO7wIAHDVhF4xe80J/fKw6ZFpSI5C749wLPn5CBqaPbjlvXXxPhVqZ/76zXZsOVSNP3yyyXhjALVN7qBspCz4EIZDxMBBb3Vote6kDc0ezP1+j2bbc1/Rqk+jS8yEGC0WqXj9VhB8sOCUqJV4/rIR+PNFQ5Ee4xVgk5PsSHXadb+pWTGoMDPog9KqX//valxzSq+InA+RqMFl7fe8tskTlKWQDbuIU2MlsRBV+3XURj3+/NU2zF1ebPq85MGHNRx2ISI/m80W1cAjU2UdGR/lqrPhiFTb6ZW7K2T39T7MlY7WNmHKXxfhlUW7InIu1H5YbbJX2+hSqfkI/C7e8cFP/ttiFsRlMfOx5Ocjls6rIYzgw2Bh7ZhoBadARLEg9vpQiuQ6FanOyHysKL/ZmZm26/Pidzux60hd3BfPotbHzIJsYq8NjxRchyUGH8uE2Vluj7nMh9o5mJ1in9ehZSqvmRbvt0zpr/o4Mx9EFDMvXj4SGckOPHrekKDn0iIafETmWMpeCJX18uCj+GgdNh2oUt23utF8oEKJxUx5k8srn0qrN+wiEjMfbp2psGrX/mLFYo1aco/38flmy2HDbVM0islbw0KOrPkgShAje3TEhkemqVa6R3LKbaQCmVrFehSV9c2QJAllNU0oyErF6c8sAgCsun8KCrJSZdtGYqYNtU9mMh9aNR2B59V/v8SAw+Vu2W9C305Yvqtctl04F/9clSaCWuK9foseZj6IEojWFLseuekRe41oLTXe6PLi7g83YOyfFsjW0dhztC5o2yaLRYWUOMxMM3W5xdkswc/PW7sfX20qDZoFI6v5OB6IdEhJQmaq/Hu+O4zibvW6MPVMTJJQ3CF+KWgNIQmDDyJCz04Zqo//ZmIfy8eKVM2H0sLtZZi3tgQA8Pf5P/sfVxsqj9TMHWp/TAUfYgZD5Xdp04Fq3PivtUHDL2LLc18A40yyo2tOmvz4Yfx+ag2lqHEK23bPDZxDhk7xeaww+CAiXDG2B/Izg9st3/eLwZaPJa6uG0lvLdvjvy0Oq0iShE0HqjDn+z3+b55NYcwECNXqPRV4a9keLpDXylkddmlya/8uKWuL5DUfLbeddhvOGJSvefwMCzPNJg3orNEUUP1nShaGXXIzkrHsnsn4/t4zotZY0Ir4nwERxV1OejJW3DclIseK1rCLSBxWkQCc88IyPPqfLfj3Twdano9Dzcelr63AHz/fYqoQkOLHTOZDHBZp1BnCU87AEjMfmw9WAwCcDju6KDIfvt/PlCQ7TuxqfnXZx2eeqNoQUCtDKQ67dEhJQreO6UFZmHhh8EFEALQ/lG+c1NfScaI17CJqli3sFXh8V1lL/Uc8C053HwmuQaHWQ+zzoZWlcsmCD+3MR2V9s+y+b7G3hmYPPlnXEghLaMl+iOqOF1OnJNktLfCWluwIWuPl7GFd0CtPfdhUHHaJdfNCIww+iEjVWScUAADunTHI0n6xGHUQh1XErpJpyQ64PN64DLtQ2yC2FleuFOsjG3bRyXwop3/7+nzUNQdmatU0upCkyFb4GoQlJzkszUhJdTpQVtMof0xnmFMcdslIiX5G0goGH0Sk6rnLRoa0X35WZJfqVhuiFzMb4jfTv3y9HZOfWYTdKjNgYkWK+tKAFA6xwZZbM/gI/H6JWbZJAzrLtjumCD5803LFoKa2ya0ZYKQk2YMCEz2pSXaUKfrf6O0uDrsw80FEbYLVlutf3nIa/nXtWPTMVU8BhypdpYZEvGgcq5OnvrUW5ooV1pu2bmLmQ2tWlFYTsRO7ZsnuK4ddPlhTgsvfWClrkFfd4IZTI0JIcdotZT6SHPagIEKvgFYcdmkNM1xEEQ8+HnnkEdhsNtm/QYOspW2JKD7MVt6f3Ktj0GMnFGXh1P55SEuO7MdKmsE3tqO1TbrPxxpnu7RuYubDpVEbpDUVNtkh//tQDrt8t60My3eV4y9fb/c/Vt3o0lxPJtlhl2UntPzz2jFYebwg/JlLhsn+TvVWqBUDGyuzamIhKpmPIUOG4NChQ/5/y5Yti8bLEFGE5aSb65745IVDcf8v1L9UDOuWg9P650XsnIyCmaO1zbrPa43rRwtjj9bNK1t5Vv1/llvjceUU1coG9d+9Q1WB7Ft9s0cn8+FAkonMxwldslCY3dLFd0hRNr66daL/Ob3JO+Lrprf3zAcAJCUlobCw0P8vLy9yH0REFD2n9jP3t5qdlowbJqrPgnE67Hjg7BMidk7pzvAyH7WNbt3nI42xR+vmNbHsvWbmQxF8VDeo/27VNQXqkP526XDt4CPJrjp1Vkk5ZCIeT2+ROLEYVW34Mp6iEnzs2LEDRUVF6NOnD6644grs27cvGi9DRBH2wDmD8ZtJffDFLafqbqeclqv89mWml4IW5UqcRrUndU36wcUVb60M+VyAliZTHEppP8TgQ6vmQ6uxmDL4qNFYwPBAZUvmIz8zBaf176yZ3WgpODX+W1F2NRWHU/TWiRncJRMDCjoAAAYUZBq+TixFPA8zduxYzJ07FwMHDsShQ4fw6KOP4rTTTsOmTZuQmRn8wzc1NaGpSSjOqa6O9CkRkUmZqU7cN8O4q6kytlCOW4cTfHRULHKXbhB8KPseKG06EPpnypGaJkx8eiHOGJyPly4/ydQ+jFNaNzHe0MpwlNepD6ekKLIU1QZZNd/MFK2i0qxUp6maD2WAIc6Q8Q0rZqUmBZ1PksOOL245DaVVjegewfWbIiHimY8ZM2bgkksuwbBhwzBt2jR8+eWXqKysxAcffKC6/ZNPPons7Gz/v+7du0f6lIgowpRFbspgQy8VbKSjou7EaJVcvSZQIo9Xslyc+uHaEjS4PPhiwyHjjY/jVNvWTTbs4g7+fzV/y2H84ZNNqvs6k+S/11qZj6D9NIZWTuufZ6rdu5I4VOPrc/Of352KW6b0x6wxPYJeu7UFHkAUMh9KOTk5GDBgAHbu3Kn6/H333Yfbb7/df7+6upoBCFErpwwulNX8jjCW8s5RZD6Mhl2MMh8AsHzXUdzz0Qbsr2jA5787FdlpTnyy7gCuGt8L2YrXE4WyJHmM61vJIrEAWW3Y5bq316juZ7cFBxE1JuuJtLIbQ7tl4/CWMlPHEIm/l97jP0/PThm4/cwBaGj2oCg7FWcOKbB83FiKevBRW1uLXbt24corr1R9PiUlBSkpkW1KRETRFZTpcEQv8xHusAsAXP7GKv/tt1cUY+uhGmw8UIV1+45hzjVjVPdZu/cYHv9iq4kzDlwAAHDcpZUzU3CqxmazBQURZouZtYLYzpkpurNVtDh0urSmJTvwO0XdVGsU8WGXO++8E4sXL0ZxcTGWL1+OCy64AA6HA7NmzYr0SxFRnChji6DMh8Yn6ue/Cy5kVfYfyEyVfycyGnapb7Y2m0WSgI0HqgAAC7cf0dzuoleWy+57dVIaYot36fg5ucNYNp2iR/z/aCX4sNuAZOWwi0Gxs49WF9NOGSmqHXzzOqTgnGFdNI8n1oDEeip5pEQ881FSUoJZs2ahvLwcnTt3xqmnnoqVK1eic+fOxjsTUZugzGwEZUI0go8hRVlBj2WkJKFOyF44HXbcfuYA/O3bnwEYNxlTtrg2IqFlefEKRVHh60t2ITcjBReP6qa6X5PbqzkEJF4AahrdOOGhr9G3cwYW3HG6pXOj6POEmvmATbN2w4hW5kPr7+TBcwZj0oDOsNtsuEjj99HH00YzbREPPt5///1IH5KIWhnlh2bQbBeNYRe1aYEdUpJk61UkOWwY2SPHfz+MiTOa+nXugNV1FQCAhdvK8MCnm/zTIy8c2VW1a2R9s1sz+BBT+av3tBx3F1e3bZXEeKNZpeBUS7PHa2pmihq1oMX3N6T2N5GS5EBOejKen2W8vhIzH0SUMJQfmB0UTZCsFJwqGygl2e1IFYZarGY2jEgS0EEY2rlm7g+y5/8+/2es21cZtF99swedNI4ZrwtAdaMLTS4vOme2/bo5j1eCxysF9dIwo7y2CYu2H8EvhnYxLFCWdDIf20trdPdVDruYpdZeXa/deYrT/HvgbaOZDy4sR0Qhe+6yEejZKR3PzRohe9xKwalysS6nwyar86ioi+zaLU1uj266/YXvdmLZzqNBjzfoTOnVCj6i3Zxs2CPf4OQn5qMqwgFaPFz0ynKMe3KB6anTol//72rcMe8nPPHlFsNtPTo1Hxe/Kq/zURY7hzzsohJQ+RaIU/tTUTYV06PVCr61Y/BBRCE7f0RXLL5rMgYVygMIs03Grj21N24+Q16Zn+SwY3CXLIzpnYsZJxZiaNfsiJ0vAHy+4RCaNBYU09OgM6tGvKCJFxOttUMi7aVF6q0M2pL1+ytRUdesmnUysvlgSyO5z030Y1HWfGw5WI3n5u9Ao8sTNHVWWfwc8rCLyn6+wMaG4L+VIV3M/8631cwHh12IyNCsMT3w3mrzyySoBR9f3Xpa0GMPnnNC0LLkSXYbHHYbPvjNeAAtTcRSnQ5sL63BvLUlFs9c3YaSSsv7qE3prahrhiRJsguamARxebz+YYQtB6uxYOthXD+xj2xYKVTirI3Xl+zGnWcNDGnIIho8XslSl1uj3huh2nWkFit2lePiUd3877lX9loSfvH80uO3g///dkhJwmEEMm8hD7uoDEP6zkeZ+Vh9/xTd3jNKbbXmo3X8phJRq/bIeSfg0tH6VfcitetOYVaq7L5vHFw5DVGZ2k51OnDdaX0wsND82hQTB3TW7Q/S6Aoh8+GSfyuubXJj8jOLMO3ZpWhsDhxPdnETMiy/eH4p/vrtz3hl0S7Lr63G5ZX/DG7h/priCmw+WBWR17Fq0fYyDH3ka3y2/oDpfcThD61l7s1Q/to99p8teODTTbj53XX+x2TBofBaPxQfCzpeZqo8CAg186FW86FVm5Kv+Dsx4mbwQUTtVUqSA6eYXPEWUK/g930TnnPNyejWMQ3/um4sAJViVY1vzFa+1d951gB8cYs802K3IawhnCZFwPLFhoOoanDhaG0TSo7V+x+XXUhVvsVHKihQDun47pdVN+LiV1fg7OeXReR1rLp6zg+ob/bg9++vN72POAzm9npD/jYv/t4t/vkIFv/c0sdl/tbDWFNcgSveXIltpYG1foz+X4nDLndNG6hau2H2vD6dfQrevX6s/7HAsEt4OOxCRO2a3uqZZvi+NU4emI9l95whe+6x84fgoc826+6vVuw3a0x3vLd6f9Dj6cmOoG+bEoBUC7MIlJo9Xtw17yes2lOBfvkd0L1jmv+5Q1WN/ttiYap6bUlk5g4rm5j5Lp7F5YFASJKksP+/+eyvqEdhdmrIRZd6ftwXyDocqGzEyU/Mx9lDu+CPM08M+ZhX/e9q2f2LX10RtI2V4GN4t5yQ2u37jOieA6Dld7DR5cXpA/MBhLcII8BhFyIiXXofssrCPjVqF72UJK2hleCGUJKkXtxnVsmxBsxbW4J9FfX4blsZ/rFir/+50upA8CHO1jjt6YV4fsEO06/h9UqmZ8gEZz5aLp5i8BOpgtelO47gtKcX4sq3VhlvHIJr5gSmO7+yaCcq6prxz5Ut72+jy4NzX1iG+z7eYHgcq/93m4X3R22ROTErl+q0qxaO9svvgGd/OQKXj+0R9NxrV44Keuzb2ybhLxcPw1XjewIALj05vLXMvJErkYkpBh9EZMqwMGedqI17+5xgorpfbX+1gKZP5wz07JSuWuQXTjfIWp1W2qVVYvAhvxr4OrX6aCUiahpdmPiXhbjhn2tRWd+M3Udqcee8n/Dkl+rryyi/qfsunmLw447QlelfxwOBlbsrDLc1k2jxeCWs318pq4nxqWuSF35uOVSNjQeq8N7q/dhYEtk6lnrh/+m+ivqg58Waj1SnQ3XYJTvNiZkju6JTRnLQc9OGFAY91j03HZeM7u6vdcpKdWLWmNADkEj9P441DrsQkSm98jLwxS2nolOGtYZWXbJTce2pvVW7hvoMLMzEG78ejXydZllq+ysfev+GcRjdsyOSHMHfUn89vic2hHHxUs7KEckyH+7gWRObDgReV+td+Os3P6PkWANKjjXgtD8vlK0bcs/0QUE/v7K/g2+WSGOYmY9wh2rsNpthkPfSwp3427c/49LR3fD0xcNlzyn7qYhFoTuP1GBoN3mgWm1yWXs1FcL/U7U+LhmKzIdaAOzryRHOexbO0EkbbfPBzAcRmTekKBuF2daq8X8/pT+uO62P4XZnnlCA4cfHxdWoNS6zC4+deUIBxvXp5P9Gqcx83P+LwbrFea+rpMhFarUlPmKTL7WXOOeFQPGn8sdYsascLy3cibnLi/2PKRcsU5t+qjXbRexHYnVxuw9+2I8xf1ogC5asMlPC4MsGfbDGeOq0WDejNkvp7nmB4Rir13/l+j5KBVmBYDglyaE69OcrYtbL7BkJJfY484QCAMB1p/YO+XXjiZkPImoT1OocxW+byloJMfgYVJiJVKdD9xtmn84dQj63ygb9i5hIWXcy642Vhvs0ub1BvUG0hl3E4aGKumaU1zVjQIG5acp3f9RyIb/jg5/w9W0TsbOsFv9auReHq9W7zLo93qCp0i0/n/mrqVGNizz4aAmsdpbV4PMNh/CrcT3x1eZS//NHa5tRWtUoCxr0LN9Vrvt8947p/tspTrtsmO8f/zMGi7cfwS3Hl68Pp3BUb8VkLa9ccRJKjjWgV15GyK8bTww+iCiqclXGwkNhV818BG4rr2FqxYFawUd6sgPZaYHx/Z6d0mG32bDnqLnF4aysPxNKdr7J7QEg7zmhNexSLXTpPPPvSwAAn//uVJxoULOzQrgQ+7Iqs95YiSM16oHHHz7ZiP/8dBDf3j4JBUJvCqs/n15TMUmSsFFoCOfLfPzl6+34evNhPKdSzPvAp5vw3GUjzL22QU8RMcvnK27+8cEz0ez2ojA7FZMGBFZrF3/um07vi/NHFJk6BwDoGMLfSJLD3mYDD4DDLkQUJS/MGonrT+uNqYMLInI8tQJSMSBRDqmINRK+DInWsEvnzBTZjJtRPTpi4Z2na56Lcj2ao7Xm158JKfhQGW4Iynwcv/+zyuJoi7aXGb6GmIHZfaQOX206pBl4AMA7q/ahutGNOd8Xyx4Xfz6jhdqcDptuq/sjNU14/rtA63hf5uPH4y3Y1f53bjlY5R9OSXbYUWRxmFDUOy8DeR2S0atTOjKP13/kZiSrDj2Kw4L3TB8UtOSAnlvO6I8zBuXjBROr2LYXzHwQUVScO7wI5w43/+3PiFHmw0zmWivzkZmaJBvWMGrcdEKXLGw6EGhWZWUSTSjTfdUu0MpiUrdHgtvjxYJthy0fX82N//rR1HYlx+ohSRIWbT+CzzccktVlTHt2Cfrnd8DXt05ULRh2eSRsPVgd9LjPbkXmqdHtgdvj1S3+LatpwsHKBgBAl5xUfPLbU3DSH7819bOIPrxxPJwOu78njV7BNBDesEt2uhP/e/XJIe/fFjHzQURtgtqHu6zmw8QxTihSH3pQdln1xSjf3DZRdfu0cNZmkS08Z64gtE5lmq9ak7GqBlfUF7NTtk3/fMMhPPHFVlwz9wd89GNwAemOslrZKsHKn+WXr2vXvCiHRV5bvBul1Y26P6NHknDgePDRNScNOWnOkLJNo3vlAmiZYmtmLZ4ROsXSFIzBBxG1CUazXfQKF6sbWmoyHj1viOrzHVLk9RS+zMeAgkzV5lHhLAxnA7C3vA43v/sj+v/hv6b2Of+l74OGMFyKLM7h6kZcpnMhjxS1tulvLtuju09lQ6AmZv+x4H4aWqoagmtpPlqrv2ZMapIDB44Fgg+73YasVPMLtQEtNT9Wje6ViznXnKw7XEcBDD6IqE0Y1CV4DF2v4FR0qKrlYpSbkYwLR3YNer5DijyYEDunqk1XTQkn+LDZ8PTX200t/y664Z9rsKa4Ajf9ay3eXbUPe8vlQxJPfbUNO8pqVfddtSfQHGxnWQ3Ofn4pPt9w0PrJH2e2C6uP+B7uLTcffKgNr2wyWBsnOcnubzFflNPSAl+tAZiWMb1zMf/2Saa3F00emI/ebbgINJZY80FEbUJuRjKW33sG0pMdGPFYyxi+OA6vV6chJgkO1zQGPe9rJvXA2YPxjxXFuO3M/v7nlLNKgPDWiPFKEpYLwxBm7S2v969P8t9NpUHPV+rMuFm64yh63fsFPrppPN5dtR+bD1bj5nfX4eyhXSBJxvUMSr98zVqGpcHlwZtLd+PDtSWyxl1G1H6mb7fo17RUNbj8wz9dj6+/k5uR7K8feX7WSFTWN+PbLYexdEfw/4fXfjUqKuvXkByDDyJqM3zfZH1ssoLT4CDh8rE98O6qfZhxYqDN9dlDi/D9znLMHFGEH4qP4UBlg78w9rrT+gQ1RFMObwDhNZSqa3JbmpobSb9/fz2mDMr33+9935cAgKsn9LJ0nNXFxm3WRct2HFUNmIyYfZ9uOr0vXlm0K+jxbjmB4MNnWNds9MrLwMrd6j0+ctKtDdFQaBh8EFGbJdZ8DFRppPXQOSfgtH55OKV/nv+xS0d3w7Bu2TihSxZqmtzYX1Gv2wOjjyKNftvUAdhVZq7/h5oyjYZdsXC0tkm1XkXsrhoNZgKP3IzkoI6jerNaOqY7/cFJ704Z+Px3p8o6yQJAF9+wS4dA8JF8vB16erL65S9SqwCTPuaWiKjNstuAz2afgmtP7Y07pw0Mej7V6cCMoV1kBYdJDjtO7JoNu92G7DSnYfOtGyf1ld2/ZUo/XHBSS93I1MH5suLECX07GZ7zlkPqU0sfOHuw4b7hanR5VWektAa9VIo8P16nXVzaJTuQBUtPcaCHYv/JAzujR27LY2Lmwxd8ZCQHgrBwl7Un65j5IKI2y26zYXj3HN01YcKVluxAn7wMf82AzWbDuD6dsOSuySjITkF5bTO+21aG80cU4UhNE8746+KQXue60/rg8S/UV7CNpKO15lvBx5LYJdUMsWeLy+P1L/AGADNOLMQrvwqs1TNEmGLtq+cQa0/SnY6g9XQoupj5IKI2S/z2G01qpaw9OqUjJcmBopw0/GpcT2SmOjVT+Uba8gyJMb1zI3Kc7rn601uVr1MjrGabmuRAslAkqlxvZvLAfGSmJqEoO9Xf00VcQTk9JYy+LRQSZj6IqM15/cpR+HFfpayQNJqMOp76hHoRU2urPXVwAa6e0AvPfLMd6/dXAgBO65/nn6Gx8ZGzsKGkCj1y03Hb/63Hmr3HQnrtcDjsNswc0RWr91grQFXTvaM8kOycmeJv7/7UhUNl04WBlgzUny4Yih+KK3DmCQWyWg2nYhglLdmB5feeAQmBIZZCcdgmOQlA/GpxEhGDDyJqc84aUoizhsQm8ADMt0/PSnXit6f3xcsqMy+0DCzIVK07SUmy49T+eTilXyf/rJRh3bKxdMdRZKYkITPViVP6tRTSPnzuEJz74rKgY0Sb02EzPe1YDJzUDOqShU9+OwGvLd6Ne2cMQq+8DEiShNLqRnTJTsMZg/PxiVAD8tdLh2Ncn06qTeDUpspmKhqNdRYyHz1y000vIkiRwWEXIiID5wzrAgAYUNDBcNu7pw/CzZP7BT0utmrvYmKxM+fxhfTEb/TpyUlY/YcpWH7fGbJtc4XZHI/PPFH1eP+++RTZ/Wd/OSKsfiVAy8Jt4uyZ30/pr7ntHMXaJWJL+0GFmRjdsyNG9uiIV68c5V+t1Waz+YfW8jNTZSsPj+ujXdyrtgihkjjs8qcLh2JC30545YqTDPejyGDwQURk4PdT++P5WSPx3vXjTG2vNntCrOu4dHR31f0W3BHorKmsWwCAoV2zkZ+ZGvQtXuzgqdaRdfLAzhjWLQefzQ4EIDnpTnx/jzyI8QVZADDnmpORk+7Edaf2Vj1XoKXoU1znZkSPHP/tt64ajRcvbxlOOntYFyQ57Jg1JvBz//CHqVh692S8MGskPp19iqkprmY7q5ppEtY9Nx13TRuIh889AV1z0vDu9eMwY2gXw/0oMjjsQkRkICXJgfMsrND7y5O747kFO2SPiRfEZGFmhiSUs/bt3EHYPnAxnn/7JGwvrcFpQr8SkZh9qGv24I/nD8GfvtyG164chaKcVPTIbQl8hnfPweMzT8SPe4/h1H55SHLYkZ7sQH1zy1L1YlAzeWA+1j14Jpo9Xs21W+qaPUgRsifZaU6s/sMU1DV5/MHW2N6d/FNdxb4sackOdM9NNyw0FZlt6m526uxslQwVxQaDDyKiCCvKScPWx6ajsqEZf/pym7+D6G8m9cGXGw/hspO7o/hoHeatLcHvpwxQPYYYrPTL74B++cZDPgBQ3+zG7Mn9MGtMD9Xsya/G9cSvxvX0359/+yS8sXQ3euSmY8qgAry3ep9/eMNmsyElyeHvFKtGDHyy05zIz0wFhH5vYm1FocXptEEMog9fXYmYYaHWicEHEVEUpCU7kJacJpvJct+MwbhvRkszsT9fNAy3nTkgqGX8mN65WL2nQnNoRsvJvTrih+JjmDmipQGaWuChpignDQ+fG1jtd8ldk5GTIR/W+dMFQ3HHmQOwdu8x5GelYuZL3/ufE6e4ijUZaq49rTc2H6zGjKGhFQsbZT7mXjMG1Q0udLSwkBzFB4MPIqI4sNttQYEHALxz3Vgcq2tGvsUswbvXj8Ox+uaWzEMYlJ1CfTp1SPHPMJo4oDOW/HwEAOAWmn0ZBR/pyUl49cpRutvoMar5cNhtDDzaCBacEhG1Ik6H3XLg4d8vzMDDrOd+OQJTBxfg6YuHydqiR3s1WJU1/qiNYuaDiIgs6ZiRjDevGu2/v/TuyUhLjn6XUMl0ySm1dgw+iIgoLFZmrITDbLM3av047EJEREQxxeCDiIjaBCY+2g8GH0RE1DYw+mg3GHwQEVGbMO34KsaDCjMNtqTWjgWnRETUJjxxwYk4uVdHTD8xdisaU3Qw+CAiojYhK9WJX4/vFe/ToAjgsAsRERHFFIMPIiIiiikGH0RERBRTDD6IiIgophh8EBERUUwx+CAiIqKYYvBBREREMcXgg4iIiGKKwQcRERHFFIMPIiIiiikGH0RERBRTDD6IiIgophh8EBERUUy1ulVtJUkCAFRXV8f5TIiIiMgs33Xbdx3X0+qCj5qaGgBA9+7d43wmREREZFVNTQ2ys7N1t7FJZkKUGPJ6vTh48CAyMzNhs9kieuzq6mp0794d+/fvR1ZWVkSP3d7wvTKP75V5fK/M43tlDd8v86L1XkmShJqaGhQVFcFu16/qaHWZD7vdjm7dukX1NbKysvjLaRLfK/P4XpnH98o8vlfW8P0yLxrvlVHGw4cFp0RERBRTDD6IiIgophIq+EhJScHDDz+MlJSUeJ9Kq8f3yjy+V+bxvTKP75U1fL/Maw3vVasrOCUiIqL2LaEyH0RERBR/DD6IiIgophh8EBERUUwx+CAiIqKYavfBx3nnnYcePXogNTUVXbp0wZVXXomDBw/q7tPY2IjZs2ejU6dO6NChAy666CIcPnw4RmccH8XFxbj22mvRu3dvpKWloW/fvnj44YfR3Nysu9/pp58Om80m+3fjjTfG6KzjI9T3KhF/rwDgiSeewIQJE5Ceno6cnBxT+1x99dVBv1fTp0+P7om2AqG8V5Ik4aGHHkKXLl2QlpaGqVOnYseOHdE90VagoqICV1xxBbKyspCTk4Nrr70WtbW1uvsk0ufVSy+9hF69eiE1NRVjx47F6tWrdbefN28eBg0ahNTUVAwdOhRffvllVM+v3QcfkydPxgcffIDt27fjo48+wq5du3DxxRfr7nPbbbfhP//5D+bNm4fFixfj4MGDuPDCC2N0xvGxbds2eL1evPbaa9i8eTP+/ve/49VXX8X9999vuO/111+PQ4cO+f89/fTTMTjj+An1vUrE3ysAaG5uxiWXXIKbbrrJ0n7Tp0+X/V699957UTrD1iOU9+rpp5/G888/j1dffRWrVq1CRkYGpk2bhsbGxiieafxdccUV2Lx5M7799lt8/vnnWLJkCW644QbD/RLh8+r//u//cPvtt+Phhx/Gjz/+iOHDh2PatGkoKytT3X758uWYNWsWrr32Wqxbtw4zZ87EzJkzsWnTpuidpJRgPvvsM8lms0nNzc2qz1dWVkpOp1OaN2+e/7GtW7dKAKQVK1bE6jRbhaefflrq3bu37jaTJk2Sfv/738fmhFoxo/eKv1eSNGfOHCk7O9vUtldddZV0/vnnR/V8WjOz75XX65UKCwulv/zlL/7HKisrpZSUFOm9996L4hnG15YtWyQA0g8//OB/7L///a9ks9mkAwcOaO6XKJ9XY8aMkWbPnu2/7/F4pKKiIunJJ59U3f7SSy+Vzj77bNljY8eOlX7zm99E7RzbfeZDVFFRgXfeeQcTJkyA0+lU3Wbt2rVwuVyYOnWq/7FBgwahR48eWLFiRaxOtVWoqqpCbm6u4XbvvPMO8vLycOKJJ+K+++5DfX19DM6udTF6r/h7Zd2iRYuQn5+PgQMH4qabbkJ5eXm8T6nV2bNnD0pLS2W/V9nZ2Rg7dmy7/r1asWIFcnJyMHr0aP9jU6dOhd1ux6pVq3T3be+fV83NzVi7dq3sd8Jut2Pq1KmavxMrVqyQbQ8A06ZNi+rvUKtbWC4a7rnnHrz44ouor6/HuHHj8Pnnn2tuW1paiuTk5KDx1oKCApSWlkb5TFuPnTt34oUXXsAzzzyju93ll1+Onj17oqioCBs2bMA999yD7du34+OPP47RmcafmfeKv1fWTJ8+HRdeeCF69+6NXbt24f7778eMGTOwYsUKOByOeJ9eq+H73SkoKJA93t5/r0pLS5Gfny97LCkpCbm5ubo/dyJ8Xh09ehQej0f1d2Lbtm2q+5SWlsb8d6hNZj7uvffeoKIh5T/xTb7rrruwbt06fPPNN3A4HPj1r38NKUEau1p9rwDgwIEDmD59Oi655BJcf/31use/4YYbMG3aNAwdOhRXXHEF3n77bXzyySfYtWtXNH+sqIj2e9WehPJeWXHZZZfhvPPOw9ChQzFz5kx8/vnn+OGHH7Bo0aLI/RAxEu33qj2J9nvVnj6v2ro2mfm44447cPXVV+tu06dPH//tvLw85OXlYcCAARg8eDC6d++OlStXYvz48UH7FRYWorm5GZWVlbJvqYcPH0ZhYWGkfoSYsfpeHTx4EJMnT8aECRPw+uuvW369sWPHAmjJBvTt29fy/vEUzfcq0X+vwtWnTx/k5eVh586dmDJlSsSOGwvRfK98vzuHDx9Gly5d/I8fPnwYI0aMCOmY8WT2vSosLAwqnnS73aioqLD099SWP6+05OXlweFwBM2k0/usKSwstLR9JLTJ4KNz587o3LlzSPt6vV4AQFNTk+rzo0aNgtPpxIIFC3DRRRcBALZv3459+/apBiutnZX36sCBA5g8eTJGjRqFOXPmwG63nhhbv349AMg+CNuKaL5Xifx7FQklJSUoLy9v979XVvXu3RuFhYVYsGCBP9iorq7GqlWrLM8uag3Mvlfjx49HZWUl1q5di1GjRgEAvvvuO3i9Xn9AYUZb/rzSkpycjFGjRmHBggWYOXMmgJbr3oIFC3DzzTer7jN+/HgsWLAAt956q/+xb7/9NrqfTVErZW0FVq5cKb3wwgvSunXrpOLiYmnBggXShAkTpL59+0qNjY2SJElSSUmJNHDgQGnVqlX+/W688UapR48e0nfffSetWbNGGj9+vDR+/Ph4/RgxUVJSIvXr10+aMmWKVFJSIh06dMj/T9xGfK927twpPfbYY9KaNWukPXv2SJ999pnUp08faeLEifH6MWIilPdKkhLz90qSJGnv3r3SunXrpEcffVTq0KGDtG7dOmndunVSTU2Nf5uBAwdKH3/8sSRJklRTUyPdeeed0ooVK6Q9e/ZI8+fPl0466SSpf//+/r/b9srqeyVJkvTUU09JOTk50meffSZt2LBBOv/886XevXtLDQ0N8fgRYmb69OnSyJEjpVWrVknLli2T+vfvL82aNcv/fCJ/Xr3//vtSSkqKNHfuXGnLli3SDTfcIOXk5EilpaWSJEnSlVdeKd17773+7b///nspKSlJeuaZZ6StW7dKDz/8sOR0OqWNGzdG7RzbdfCxYcMGafLkyVJubq6UkpIi9erVS7rxxhulkpIS/zZ79uyRAEgLFy70P9bQ0CD99re/lTp27Cilp6dLF1xwgezC0h7NmTNHAqD6z0f5Xu3bt0+aOHGi//3t16+fdNddd0lVVVVx+iliI5T3SpIS8/dKklqmzaq9V+J7A0CaM2eOJEmSVF9fL5111llS586dJafTKfXs2VO6/vrr/R+c7ZnV90qSWqbbPvjgg1JBQYGUkpIiTZkyRdq+fXvsTz7GysvLpVmzZkkdOnSQsrKypGuuuUYWpCX659ULL7wg9ejRQ0pOTpbGjBkjrVy50v/cpEmTpKuuukq2/QcffCANGDBASk5OloYMGSJ98cUXUT0/myQlSOUlERERtQptcrYLERERtV0MPoiIiCimGHwQERFRTDH4ICIiophi8EFEREQxxeCDiIiIYorBBxEREcUUgw8iIiKKKQYfREREFFMMPoiIiCimGHwQERFRTDH4ICIiopj6f+eXzG/Jo+MXAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lri, lossi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph shows that -1.0 (which would result in a learning rate of 0.1 -- 10**-1 = 0.1) is a really good learning rate. It graph seems to flatten out a bit there before blowing up again. So, our orignal learning rate was a good choice but now we have confidence (and proof) of that. We can remove the learning rate code and just manually put in 0.1 and run the model for much longer.\n",
    "\n",
    "Note: learning rate decay is where you change your learning rate (decrease by a factor) in the late stages of training to further refine the model. In this case, we will change the learning rate to 0.01 (reduced by a factor of 10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0147757530212402\n"
     ]
    }
   ],
   "source": [
    "# for tracking learning rate stats\n",
    "# lri = []\n",
    "# lossi = []\n",
    "\n",
    "for i in range(10000):\n",
    "    \n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, X.shape[0], (32,)) # creating a minibatch of size 32\n",
    "    \n",
    "    # calculating loss (forward pass)\n",
    "    emb = C[X[ix]] # (32, 3, 2)\n",
    "    h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # (32, 100)\n",
    "    logits = h @ W2 + b2 # (32, 27)\n",
    "    # counts = logits.exp()\n",
    "    # prob = counts / counts.sum(1, keepdim=True)\n",
    "    # loss = -prob[torch.arange(32), Y].log().mean()\n",
    "\n",
    "    # we can replace the above lines with a more condensed and efficient version\n",
    "    # it doesnt create new tensors like we do above but uses fused kernels for efficiency\n",
    "    # this also makes the backward pass more efficient bc the formulas can be simplified\n",
    "    loss = F.cross_entropy(logits, Y[ix])\n",
    "    # print(loss.item())  \n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    # update parameters\n",
    "    # lr = lrs[i]\n",
    "    lr = 0.01\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad # manual value is learning rate\n",
    "        \n",
    "    # track stats for learning rate\n",
    "    # lri.append(lre[i])\n",
    "    # lossi.append(loss.item())\n",
    "print(loss.item())  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is performing better than the previous bigram model (which acheived a loss of about 2.5 compared to the almost 2.0 loss here)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is the Model Really Better?\n",
    "As the model gets bigger and bigger, the chance of overfitting on the training set.\n",
    "\n",
    "Overfiting: the loss will get very low (maybe even 0) but all the model is doing is simply memorizing the training data. If you try and sample from this model, you will only get examples exactly from the training set. Additionally, if you try and evalute loss on a different set (of novel data) the loss may be very high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Splits\n",
    "This means you need to split the training data into 3 different sections.\n",
    "\n",
    "1. Training split -- 80% -- used to optimize parameters using gradient descent\n",
    "2. Validation/dev split -- 10% -- development over all the hyperparameters (size of embiddings, hidden layers etc.)\n",
    "3. Test split -- 10% -- evaluate performance after training (use sparingly to avoid training on the test split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now going to break up our data into these three splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182580, 3]) torch.Size([182580])\n",
      "torch.Size([22767, 3]) torch.Size([22767])\n",
      "torch.Size([22799, 3]) torch.Size([22799])\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "\n",
    "# builds a dataset only on the words passed in\n",
    "def build_dataset(words):\n",
    "    block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "    X, Y = [], []\n",
    "    for w in words:\n",
    "        \n",
    "        # print(w)\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            # print(''.join(itos[i] for i in context), '--->', itos[ix])\n",
    "            context = context[1:] + [ix] # crop and append\n",
    "    \n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y\n",
    "\n",
    "# randomly shuffling the words\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "\n",
    "# 80% of the words\n",
    "n1 = int(0.8*len(words))\n",
    "\n",
    "# 90% of the words\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "# build training set\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "\n",
    "# build dev set\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])\n",
    "\n",
    "# build test set\n",
    "Xte, Yte = build_dataset(words[n2:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will incorporate these sets into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training set\n",
    "Xtr.shape, Ytr.shape # dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing neural network using a generator\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C = torch.randn((27, 2), generator=g)\n",
    "W1 = torch.randn((6,100), generator=g)\n",
    "b1 = torch.randn(100, generator=g)\n",
    "W2 = torch.randn((100, 27), generator=g)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "parameters = [C, W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for tracking learning rate stats\n",
    "lri = []\n",
    "lossi = []\n",
    "\n",
    "for i in range(1000):\n",
    "    \n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, Xtr.shape[0], (32,)) # creating a minibatch of size 32\n",
    "    \n",
    "    # calculating loss (forward pass)\n",
    "    emb = C[Xtr[ix]] # (32, 3, 2)\n",
    "    h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # (32, 100)\n",
    "    logits = h @ W2 + b2 # (32, 27)\n",
    "    # counts = logits.exp()\n",
    "    # prob = counts / counts.sum(1, keepdim=True)\n",
    "    # loss = -prob[torch.arange(32), Y].log().mean()\n",
    "\n",
    "    # we can replace the above lines with a more condensed and efficient version\n",
    "    # it doesnt create new tensors like we do above but uses fused kernels for efficiency\n",
    "    # this also makes the backward pass more efficient bc the formulas can be simplified\n",
    "    loss = F.cross_entropy(logits, Ytr[ix])\n",
    "    print(loss.item())  \n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    # update parameters\n",
    "    lr = lrs[i]\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad # manual value is learning rate\n",
    "        \n",
    "    # track stats for learning rate\n",
    "    lri.append(lre[i])\n",
    "    lossi.append(loss.item())\n",
    "# print(loss.item())  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
